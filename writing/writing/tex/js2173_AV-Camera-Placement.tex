%% 
%% ACS project dissertation template. 
%% 
%% Currently designed for printing two-sided, but if you prefer to 
%% print single-sided just remove ",twoside,openright" from the 
%% \documentclass[] line below. 
%%
%%

\documentclass[a4paper,12pt,twoside,openright]{report}



\def\authorname{Joshua G. Send\xspace}
\def\authorcollege{Trinity Hall\xspace}
\def\authoremail{js2173@cam.ac.uk}
\def\dissertationtitle{Offboard Camera Placement for Autonomous Robot Navigation}
\def\wordcount{TODO}


\usepackage{epsfig,graphicx,parskip,setspace,tabularx,xspace} 
\usepackage[
    backend=biber,
    style=numeric,
    sorting=ynt
]{biblatex}
\addbibresource{bibliography.bib}

\usepackage{amsmath}
\usepackage{gensymb} 
\usepackage{wrapfig}
\usepackage{physics}
\usepackage{pgfplots}
\pgfplotsset{compat=1.3}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bm}

\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\newcommand{\etal}{\textit{et al.}}
\newcolumntype{N}{>{\centering\arraybackslash}m{.7in}}

\DeclareMathOperator{\arctantwo}{arctan2}
%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing

%% START OF MAIN TEXT 

\chapter{Introduction}
\pagenumbering{arabic} 
\setcounter{page}{1} 

\section{Research Vision}

It is conceivable that, in the near future, private vehicles have been made redundant by an efficient
fleet of local autonomous vehicles, ferrying passengers on demand between locations in 
a city. The details of the implementation could vary -- the current, conventional
approach is to use highly perceptive vehicles that completely sense their environment independently.
I propose an alternative direction, where the city manages a public network of
cameras, helping vehicles localize themselves in the streets and sense their environment.

To imagine further what this might look like, a city could offer a pool of robots
consisting of vehicles from various manufacturers, managed by a public-private partnership. 
Much like a utility, this service comes to be seen as an essential 
part of the city infrastructure fulfilling the needs of its citizens.

By the time this vision could be implemented, object detection algorithms would be
highly accurate, enabling cameras to identify pedestrians and vehicles to within a few centimeters. 
These locations would be transmitted to nearby vehicles. As a result, GPS, radar, and expensive LIDAR systems
would be largely redundant on the autonomous vehicles.

Following from this, autonomy becomes cheaper to implement,
and more accessible. Vehicle manufacturers work with the utility to keep the cameras 
functional and up to date. City GPS canyons are avoided, and autonomy is enabled 
on everything from bicycles, to small delivery robots, to standard vehicles -- all by 
shifting the sensing task from the individual user to the infrastructure.

\section{Motivation}
\label{sec:intro:motivation}

The core task of an autonomous vehicle is to navigate correctly and safely from a start
to a destination. This process can be split into major subtasks consisting
of path planning, localization, and safe navigation.

Current autonomous vehicle projects use a large set of sensors, combined with
maps, to solve each of those complex tasks. This comes at a cost: current
Level 3 attempts for example use LIDAR, which 
provides rich data to perform self-localization and dynamic obstacle avoidance,
but also costs around \$75,000~\cite{lin2018architectural}. The computational
power required to fuse and process data from wheel odometry,
GPS, inertial measurement units (IMUs), cameras, RADAR, LIDAR is also substantial
and generally implies only large platforms can be used.

I propose moving the localization and navigation sensing tasks
from vehicles to the infrastructure, which is particularly applicable in urban environments.
I examine a static environment containing a single vehicle, with dynamic components and safety
as future research avenues.

Most current localization approaches utilize global satellite navigations systems,
such as GPS and its augmentations like real time kinematic (RTK)~\cite{scherzinger2000precise}.
The US government reports a global 95\% accuracy of 0.715 meters~\cite{USGPSPerformance}. 
However, GPS errors are introduced in urban environments -- according to~\citeauthor{miura2015gps}, even with the
authors' proposed correction steps, the mean localization error was around 5.2 meters in urban canyons. 
In contrast, most authors agree autonomous vehicles can tolerate 0.2 to 0.35 meters lateral error
~\cite{vivacqua2017low}\cite{ziegler2014video}\cite{mattern2010high}.

To achieve these error bounds, current techniques may use computationally heavy visual odometry~\cite{ziegler2014video},
very high precision maps~\cite{mattern2010high}, or expensive LIDAR to localize
lane markings very accurately~\cite{hata2014road}.  

In this work, I show that if certain installation and algorithmic constraints
can be met, cameras mounted at side of roads can provide the required localization
accuracy without reliance on GPS, LIDAR, or radar. I also build a simulation
of an autonomous vehicle which utilizes wheel odometry-based dead reckoning 
to navigate between updates received from the infrastructure. In this context,
I then examine the task of optimal camera placements and configuration for localization and navigational performance.

The results explored here could also be useful in other scenarios, such as indoor
robotic localization (eg. in factories), or in other constrained environments.

\section{Contributions}

The practical questions answered by this work are the following:
%in the context of the vision presented previously, are
%the ones asked by city planners when designing camera placements: 
\begin{enumerate}
    \item How accurate can offboard (ie. in the environment) cameras be for localization tasks?
    \item In which direction should the camera be facing and what should its properties be (for instance, field of view)
          to optimally aid vehicle localization and navigation, and how is related to road curvature?
    \item How should a set of cameras be placed along a path to optimize navigational performance?
\end{enumerate}

The first question relates to feasibility of direct camera-based localization,
and is addressed at the end of Chapter~\ref{chap:cameramodel}.

The second and third questions examine the optimal camera placement task 
for localization and navigation performance. This is related to past work 
on landmark placement for robotic navigation, where the robots observe landmarks fixed to the environment. 
Here, the use case is inverted to observe the robot as it moves through the environment. 
Thus, it is also closely related to previous work on surveillance networks.

While literature on surveillance using cameras has explored camera placement, few
has allowed more degrees of freedom than simply adjusting the tilt (pitch) 
and choosing one of a limited set of positions. Further, no exploration of 
properties of cameras has been performed - for instance, the impact
of allowing a larger field of view in the observation task. Lastly, surveillance
tasks normally seek to maximize different objective functions than robotic
navigational performance.

Finally, throughout this work I examine the impact of the environment, specifically
road curvature, on the various tasks presented. To my knowledge, this variable has not been
analysed previously.

In summary, I make the following contributions in this work
\begin{itemize}
    \item An error model for vehicle localization using infrastructure-mounted cameras, which is motivated by physical properties such as installation uncertainty
    \item A feasibility analysis for using cameras for localization, using the developed error model
    \item An analysis of placement of a single camera camera along roads, and the relationship to road curvature
    \item An analysis of placement of sets of cameras along roads, and the relationship to road curvature
\end{itemize}



\section{Overview}

Chapter~\ref{chap:relatedwork} reviews relevant literature in the fields of
localization, landmark placement, and surveillance. Chapter~\ref{chap:impl} presents 
the simulation used for analysis, as well as background material referenced throughout 
this dissertation.

Chapter \ref{chap:cameramodel} then develops the error model for cameras
used in simulation, and examines the feasibility of using cameras in the infrastructure for localization.
Chapter ~\ref{chap:cameraplacement} optimizes single and multiple camera placements
in various environments and using different objective functions. This is followed by concluding remarks.

\chapter{Related Work} 
\label{chap:relatedwork}

%\cite{betke1997mobile} Early work on landmark-based localization.
%Use bearing-based triangulation, analyze the impact of 
%error in measurements on localization. Use least-squares
%to find a best fit, then measure error, and analyze
%how it grows as measurement grows, similar to what I do with
%cameras, given an installation and algorithmic inaccuracy.

This work touches on a wide variety of fields, from control
theory to computer vision to landmark selection. I focus
on selected works which motivate the problem being tackled here.

\section{Offboard Localization}

A core idea underlying this dissertation is the use of
external sensors to correct the localization of a mobile robot. 
This is hardly a new idea -- sources dating back to the 90s and before have looked at 
using infrastructure-based sensors for localization,
navigation and tracking.

The choice of sensor however varies widely. Many researchers
have used bearing or distance-based triangulation methods combined
with ultra-wide band (UWB), sonar, angle-of-arrival or other wireless
signals to perform localization TODO CITE. These all suffer
from the same drawbacks: measurements can be unreliable
due to delayed signals or multipath effects, and plugging
into these systems requires specialized hardware. Received-signal-strength techniques, 
which usually map the environment beforehand, then look up the current
signal fingerprint, are limited to well mapped environments
and suffer badly in dynamic environments. TODO CITE

Some passive approaches have been attempted as well, such as
providing the vehicle or the environment with RFID tags that
can be scanned and matched to a premade map. According to 
civil engineering researchers at the Cambridge Department of Engineering,
these approaches have generally been deemed a failure due to 
maintenance issues, and lack of adaptability. CITE TODO

External, vision-based monitoring systems found some support
in the 90s and early 2000s. The best known work might be
from~\citeauthor{kruse1998camera}\cite{kruse1998camera}, who implement a network
of cameras in an indoor environment to track and localize robots.
They utilize a set of LED's mounted on the corners of the robot
to aid in this, and show that they can reduce the robot's
positional error. Later,~\citeauthor{menegatti2005distributed}\cite{menegatti2005distributed}
use a slightly different approach of matching images taken
by the infrastructural cameras to those taken by the robot to
perform localization. However, since that 2005 paper external 
camera-based localization appears to have fallen out of favor,
only recently appearing slightly differently
in the context of surveillance tasks (see the following section).

The lack of further development of offboard camera localization
is likely to be caused by two main difficulties: 
firstly, the cost of instrumenting the environment -- but note
that many cities already have extensive camera networks.
Secondly, a major problem in the past was the inability to acquire
the agent without visual markers on the robot. In the authors
own words ``... it is not easy to correctly detect the
robots in dynamic environments''. I believe with the recent 
breakthroughs in machine learning-based object recognition 
and segmentation, it is time to revisit the option of external cameras for localization. 

There are added benefits to using cameras as well: they are more immune
to crosstalk and interference than radio-based techniques, and provide
a large amounts of salient information. This can include 
information about other objects in the scene, such as pedestrians,
bicycles, etc. And, even though in this work I only consider
the $(x,y)$ world coordinates of the vehicle, images could also be used
to update the heading of the robot and correct for orientational drift.




%Some related work in sensor selection, which minimizes the cost
%of acquiring a data point/selecting a sensor to use, but these
%normally involve energy or communication optimizations, as well 
%as imply some redundancy in the network -- by design, we will
%place as few cameras as required or only have a limited number~\cite{rowaihy2007survey}.


\section{Camera and Landmark Placement}

A second theme is the optimization of sensor placement. This problem is addressed
in surveillance literature, computational geometry, and robotics, among other fields.

Perhaps the most direct predecessor to this work is the PhD thesis of~\citeauthor{beinhofer2014landmark}\cite{beinhofer2014landmark},
which focuses on landmark placement for mobile robots. The key difference
is the use of an external sensor rather than a camera mounted on the robot.
Additionally, he primarily considers circular fields of view on the robot, looking
straight up, whereas I consider optimizing for position, orientation,
and to an extend field of view. However, many of the formulations, including
the Monte Carlo approach to evaluating mutual information (Section~\ref{sec:cameraplacement:mutualinformation}).
A noteworthy contribution of this work is the use of submodularity (diminishing returns
on placements) to prove good approximations to the NP-hard landmark placement problem.

Another relevant paper from the surveillance literature is~\cite{bodor2007optimal}.
Although the authors formulate a completely different objective function intended
to maximize the observed length of piecewise-linear paths,
their ideas inspired my computationally cheap objective function presented 
in Section~\ref{cameraplacement:cheap}. The authors use their objective
to place a series of cameras with two degrees of freedom (location
on the border of a region, and pitch). I use my metric to reduce 
the search space across four degrees of freedom (pitch, yaw, and vertical and
horizontal field of view) into a few interesting points, 
which are then passed onto a more complex metric based on~\citeauthor{beinhofer2014landmark}'s thesis.

In~\cite{horster2006optimal}, the authors optimize camera coverage of a map,
modeling camera coverage as a 2D triangular region. They propose using binary
mixed integer programming to find the global optimum of multiple camera orientations.
In~\cite{wang2013achieving} the authors optimize directional coverage, that is
the fraction of the time cameras can achieve a specific view of the object, defined
by an orientation and an allowed angle about it.~\cite{osais2010directional} defines
a general sensor network coverage problem, which optimizes generic directional sensors'
field of view and yaw using linear programming, but specifically consider ground-based
sensors rather than cameras. Please refer to~\cite{guvensan2011coverage} for more work
on camera-based coverage models. Common to these works it their 2D treatment of the placement
problem, optimizing for coverage rather than localization power,
as well as using expensive linear programming-like optimizers.


Lastly, it is worth mentioning the work from~\citeauthor{bansal2014understanding}\cite{bansal2014understanding}, 
who consider the problem of camera orientation on autonomous vehicles themselves.
They show that the most informative region (defined by entropy) is given when the 
camera maximizes useful pixels -- ie. focuses the most pixels on informative regions
that have distinguishing characteristics, mostly along the edge of roads.


***** TODO *****
Double check to make sure no one's looked at curvature and effect on sensor placement

Maybe an image in here? It's a bit full of text


\chapter{Design and Implementation}
\label{chap:impl}

This section outlines the work completed during the development
of this dissertation, and provides mathematical background
referenced to throughout the report.

TODO insert image of Prius model to break up the text a bit
 
\section{Approach}

The basis of this work is a simulation developed using industry-standard tools.
The Gazebo~\cite{koenig2004design} robotics simulator is used in conjunction with ROS,
the Robot Operating System~\cite{quigley2009ros}, to model a non-holonomic Ackermann drive vehicle
following predefined paths. A camera is modeled using standard formulations
discussed in Section~\ref{impl:sensors}.

An expensive, high fidelity simulation was used instead of a real hardware
implementation. Simulation allows quickly modifying the environment
and models to examine a greater range of tasks.


\section{Cameras}
\label{impl:sensors}

The standard model for cameras is the pinhole projective model. This model 
keeps lines in the real world as straight lines in the pixel plane,
and can be implemented as a single matrix multiplication

TODO matrices describing world => pixel transformation

My implementation is a raytracing module that only uses the
translation and rotation portions of the matrices above, which bring the world into place
in the camera's zero-centered, Z-axis aligned coordinate system. Raytracing
allows adding parametric geometry into the world, which is used later.
Additionally, my formulation can also be used to model non-pinhole camera models,
including ones that better represent ultra-wide field of view cameras. However, this work leaves
examining the impact of such cameras on robot localization and navigation as
a future research direction; I cap the maximum field of view of the pinhol model
at 110 degrees horizontally and vertically, when the perspective model
breaks down~\cite{fleckperspective} (some authors claim different cutoffs all the way down
to 70 degrees~\cite{sharpless2010pannini}, but for the analysis presented here it won't 
matter too much). 

An infrastructure-mounted camera's task is to localize vehicles in its pixel plane 
into world coordinates, and provide an error bound. The error bound is developed 
in Chapter~\ref{chap:cameramodel}. To localize the vehicle, the camera needs to
be provided with its own world orientation and location, as well as its parameters.
\textit{Camera parameters} will be used to mean both the horizontal
and vertical field of view of the camera, as well as its resolution. 

The parameters, location, orientation and a vehicle in the pixel plane can be
combined to estimate the geometric center of the vehicle, on the ground plane. Thus,
the update from the camera is world location $(x,y)$, as I assume a planar surface
with $z = 0$.

In the following chapter I discuss how I model an abstract localization algorithm. A 
concrete implementation could use an object detection or segmentation network,
such as YOLO~\cite{redmon2018yolov3} or DeepLab~\cite{chen2017rethinking}, which are constantly
improving, to determine a bounding box for the vehicle. The center of the box in the pixel plane
can be seen as an estimate of the volumetric center of the vehicle. 
Using an estimate of the height of the vehicle, a geometric correction can be applied 
to obtain a ground estimate of the vehicle position. %TODO maybe put this experiment in the Appendix?

However, real implementations could be arbitrary complex algorithms,
or even require vehicles carry visual markers.

\subsection{Coordinate Systems}
For reference, the following graphic shows the orientations
associated with camera placements. I only allow pitch and yaw
to be modified during my optimizations, as it simplifies
the geometric modeling performed in following chapters. 

TODO FIGURE


\section{Road Representations}

One key aspect of this work is that the desired trajectory is known.
There are a variety of ways of representing paths that have been
used in the literature.

One simple option is to discretize any curve as a set of linear
piecewise segments such as in~\cite{bodor2007optimal}, where it was used to 
represent paths to observe in a surveillance task. This is attractive
since any arbitrarily complex path can be represented the same way,
with some tradeoffs in accuracy and size. 

I chose to use a slightly more complex representation: piecewise
constant curvature paths, also known as Dubins curves~\cite{dubins1957curves}.
They have an easy geometrical definition, where curvature $C = 1/R$, $R$ 
being the radius of the circle with the given curvature.

TODO figure of dubins curves - original path I used?

Dubins curves can therefore be easily parametrized using only curvature and length.
My implementation allows specification sequences of curvatures and lengths, which are
then joined together (again using translation and rotational matrices) to form
continuous curves. Dubins curves also lend themselves to analytically finding the closest point
on the curve to any other world point. I used this property to calculate
the perpendicular error distance used for the steering controller
discussed in~\ref{sec:impl:vehicle}.

Alternative representations were considered, including splines and
Euler curves (linearly varying curvature paths),
but these were deemed overly complex for the purpose of this work.

\subsection{Implementation}

I began representation of rotations as
quaternions and translations as vectors, but this was very error prone. 
In the end I used homogenous coordinates to jointly store translation
and rotation matrices, and unit tested all geometric transformations.

In order to provide road boundaries and 
maintain compatibility with placement of cameras, I define
constant-width roads (3 meters), with an additional side offset that
might represent pavements (1.5 meters). Cameras can be placed at the 
side offset. I then place parametric cylindrical walls
or planes in the ray-tracing space to prevent cameras from
seeing beyond the edges of roads.


\section{Vehicle Model and Controls}
\label{sec:impl:vehicle}

The navigation tasks performed throughout are executed by a modified
open-source model of a Toyota Prius~\cite{osrfPrius}. This model
takes two primary commands: throttle, and steering wheel angle.
The state of the robot is both a world position $(x,y,z)$ and
and orientation quaternion, but due to the planar restriction
the state we are interested in can be written as
$(x,y, \theta)$, where heading $\theta$ is the angle from the $x=0$ axis 
on the ground plane.

\subsection{Controllers}

\subsubsection{PID Controller}
PID controllers~\cite{aastrom1995pid} are widely used feedback controllers
that calculate a response based on an error signal, its integral, and its derivative
(hence, \textbf{P}roportional, \textbf{I}ntegral, \textbf{D}erivative).

TODO figure of PID feedback

To use a PID controller, one needs to define target value and a current state,
which together form an error. Then, the PID gains are tuned to match
the response of the system.

Typical downsides of PID are that they only control a single variable, their
parameters can be difficult to tune, and that they don't perform
well in out-of-ordinary scenarios. I implemented a PID controller to 
maintain vehicle speed, and attempted to use another for steering angle,
which proved too difficult to tune well for wider variety of scenarios.
 
\subsubsection{Speed Controller}

The speed PID controller has access to the vehicle throttle 
and the current vehicle speed. Thus, I need to define
a target speed for all points in time to create an error function.

I set target speed policy $s$ based on the curvature $C$ of the 
closest point on the reference path:

\begin{flalign}
    s(C) &= 
    \begin{cases} 
      20.0   & C = 0.0 \\
      \frac{k}{C} & C > 0.0 
   \end{cases}
\end{flalign}

Constant $k$ was experimentally found to perform well at $k=0.5$.
20.0 m/s was also found to be a reasonable straight-line speed, which corresponds
to about 72 km/h.

However, just using $s(C)$ as the target speed results in large overshoots
at tight corners -- velocity controllers practically require some amount of lookahead
due to finite decelerations. Therefore, I define a target speed as the mean of 
$s(C)$ at $n$ equidistant points ahead of the vehicle on the curve.
This induces a linear deceleration up to a curve. I also only allow accelerations
once the curvature decreases.

\begin{figure}
\centering
\begin{subfigure}{0.55\textwidth}
    \centering
    \input{figures/implementation/velocity_controller.pgf}
    \label{fig:ekf:speed controller}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
    \centering
    \input{figures/implementation/calibration_circuit_plain.pgf}
    \label{fig:ekf:calibration plain}
\end{subfigure}
\caption[Speed Controller]{Speed Controller Performance over the calibration curve, traversed from (0,0) counter clockwise. Notice the sharp
corner in the top right corner corresponds to a big drop in target and actual speed.}
\label{fig:impl:speedcontroller}
\end{figure}



\subsubsection{Steering Controller}

The steering controller must minimize deviation from the desired trajectory
as much as possible. PID controllers are difficult to use for this task
since they tend to oscillate or underreact, or perform well at once
constant speed but not others.

Instead, I use the steering controller published in~\cite{thrun2006stanley}, which
was describes the autonomous vehicle that won the 2006 DARPA Grand Challenge. They define
a controller that directly computes the desired steering angle using only the 
perpendicular distance to the path, as well as the vehicle's speed.

TODO image of crosstrack distance, angles

\begin{flalign}
    \delta(t) &= \Psi(t) + \arctan\frac{ke(t)}{u(t)}
\end{flalign}

The original paper shows that when used with
with a linear bicycle model, the crosstrack error $e(t)$ converges exponentially to zero. The constant $k$ 
was experimentally determined to perform well at $k = 0.95$ across a wide variety of paths.
Some overshoot can be found when entering mid-curvature bends, which is a 
consequence of using two disjoint controllers for steering and
velocity. A more sophisticated model might use model predictive
control for all required controls~\cite{borrelli2005mpc}. Vehicle
control is a complex task in itself with extensive literature, refer to~\cite{paden2016survey}
for a comprehensive survey. The approaches employed here
were chosen mostly for their computational simplicity.


\subsection{Extended Kalman Filter}
\label{chap:impl:vehicle:ekf}

The simulated vehicle tracks its position using an Extended Kalman Filter (EKF)
to integrate camera updates and correct its inaccurate
dead reckoning~\cite{fujii2013extended}. An EKF
operates in predict and update steps, propagating the vehicle's state
according to a motion model in the predict step, and correcting
the vehicle state during the update step if there is a measurement present. Like a standard Kalman Filter,
every step produces normal distributions, which are efficient to compute over.
The main difference to a standard Kalman Filter is that non-linear motion models
can be used by linearizing the model at each timestep with a
Taylor-like expansion about the estimated mean.

Formally, if the previous state of the vehicle is $\bm{x}_{t-1} = (x_{t-1}, y_{t-1}, \theta_{t-1})$ and
covariance is $\bm{\Sigma}_{t-1}$, the predict step is given as

\begin{flalign}
    \bm{\hat{x}}_{t} &= f(\bm{x}_{t-1}, \bm{u}_{t}) \\
    \bm{\hat{\Sigma}}_{t} &= \bm{F}_t \bm{\Sigma}_{t-1} \bm{F}_k^T + \bm{Q}_k
\end{flalign}

Here, $f$ is the vehicle motion model described in the following section, $u$ are control commands,
and $\bm{Q}$ represents process noise. $\bm{F}$ is the Jacobian of the motion model, 
evaluated at the prior timestep -- this is the linearization step of the EKF
that distinguishes it from a normal Kalman Filter.

The update step is then performed as the following, given a measurement
$\bm{z}_t$.

\begin{flalign}
    \bm{\tilde{y}}_t &= \bm{z}_t - h(\bm{\hat{x}}_t) \\
    \bm{S}_t &= \bm{H}_t \bm{\hat{\Sigma}}_t \bm{H}_t^T + \bm{R}_t \\  
    \bm{K}_t &= \bm{\hat{\Sigma}}_t \bm{H}_t^T \bm{S}_k^{-1} \\ 
    \bm{x}_t &= \bm{\hat{x}}_t + \bm{K}_t \bm{\tilde{y}}_t \\
    \bm{\Sigma}_t &= (\bm{I} - \bm{K}_t\bm{H}_t)\bm{\hat{\Sigma}}_t
\end{flalign}

Here, $h(\bm{x})$ is the \textit{sensor model}, which describes
the relationship between a state and a produced measurement.
$\bm{H}$ is the Jacobian of $h(\bm{x})$, which for my 
purposes I set as the identity (ie. a linear relationship,
which works well enough in practice). Finally, $\bm{R}$ is
the covariance of the measurement.

An important value is the \textit{Kalman Gain}, $\bm{K}$.
In an intuitive sense, the update step can be seen as
performing a weighted average between the vehicle's state,
and the incoming measurement. The weighting $\bm{K}$
is determined by a relationship analogous to $\frac{\bm{\Sigma}}{\bm{\Sigma} + \bm{R}}$,
if the these matrices were scalars. I come back to this idea
in Chapter~\ref{chap:cameraplacement}.



\subsubsection{Odometry Motion Model}
\label{sec:impl:motion model}

I describe the probabilistic motion model $f(\bm{x}, \bm{u})$
that is used as the basis of the EKF predict step~\cite{thrun2005probabilistic}.
Here, the control commands $\bm{u}$ are not actually the commands sent to the vehicle,
but taken from the ``wheel odometry'', which is just the simuation's ground
truth state (a common formulation). So, $\bm{u}_t = (\bar{x}_t, \bar{y}_t, \bar{\theta}_t)$ is the vehicle's
true state at time t.

The high level explanation of the motion model is as follows: I use the vehicle's ground truth movement 
between timesteps to calculate a series of deltas:
the translational change, and a starting and ending angular change. These 
are perturbed slightly and added to the EKF's current state.

TODO figure 

\begin{flalign}
\delta^{trans}_{t} &= \sqrt{(\bar{x}_{t+1} - \bar{x}_t)^2 + (\bar{y}_{t+1} - \bar{y}_t)^2} \\
\delta^{rot1}_{t} &= \arctantwo(\bar{y}_{t+1} - \bar{y}_t, \bar{x}_{t+1} - \bar{x}_t) - \bar{\theta}_{t+1} \\
\delta^{rot2}_{t} &= \bar{\theta}_{t+1} - \bar{\theta}_{t} - \delta^{rot1}_{t}
\end{flalign}

This calculates the ground truth changes to orientations and distance.

We next add noise to these deltas using a function $norm(\mu, \sigma^2)$ which 
samples the indicated normal distribution. 

\begin{flalign}
\hat{\delta}^{rot1}_t &= \delta^{rot1}_t + norm(0, \alpha_1|\delta^{rot1}_t| + \alpha_2|\delta^{trans}_t|) \\
\hat{\delta}^{trans}_t &= \delta^{trans}_t + norm(0, \alpha_3|\delta^{trans}_t| + \alpha_4|\delta^{rot1}_{t} + \delta^{rot2}_t| \\
\hat{\delta}^{rot1}_t &= \delta^{rot2}_t + norm(0, \alpha_1|\delta^{rot2}_t| + \alpha_2|\delta^{trans}_t|) 
\end{flalign}

Lastly, given current mean belief state $(x_t, y_t, \theta_t)$, we generate

\begin{flalign}
x_{t+1} &= x_t + \hat{\delta}^{trans}_t\cos(\theta_t + \hat{\delta}^{rot1}_t) \\
y_{t+1} &= y_t + \hat{\delta}^{trans}_t\sin(\theta_t + \hat{\delta}^{rot1}_t) \\
\theta_{t+1} &= \theta_t + \hat{\delta}^{rot1}_t + \hat{\delta}^{rot2}_t 
\end{flalign}

This creates the updated state $\bm{x}_{t+1}$. The Jacobian $\bm{F}$ required
for the EKF prediction step is computed from the partial derivatives
of these three equations.


\subsection{Calibration}

There are four constants, $\alpha_{1-4}$ which are present in the odometry
model equations. These determine the ``shape'' of the robot's probabilistic
motion.

\begin{figure}
    \includegraphics[width=\linewidth]{figures/implementation/ekf/noise_params.png}
\caption[Noise Parameter Settings]{How various settings of $\alpha$'s affect the motion model. The canonical settings produce the leftmost spread\textsuperscript{1}}
\tiny\textsuperscript{1}{From \url{http://ais.informatik.uni-freiburg.de/teaching/ss11/robotics/slides/06-motion-models.pdf}}
    \label{fig:ekf:noiseparams}
\end{figure}

I use settings which produce canonical distributions, increasing lateral uncertainty more
than longitudinal uncertainty.

%TODO Example of state propagation along a line with a sharp turn

Lastly, since I am using a simulation without a physical reference vehicle to
calibrate my parameters against, I use data from~\cite{vivacqua2017low}. 
They measure their vehicle's wheel odometry over 12 laps of a circuit, 1800 meters long, similar
to the one below. I mainly aim to produce similar order-of-magnitude errors.

\begin{figure}
    \begin{center}
        \input{figures/implementation/calibration_circuit.pgf}
    \end{center}
    \caption[Calibration Circuit]{The calibration circuit containing one tight turn, two middle turns, and one low curvature turn, for a total length of about 1800 meters.}
    \label{fig:ekf:noiseparams}
\end{figure}

I ran my simulation on the circuit 20 times, and compare my calibrated results to~\cite{vivacqua2017low} in
Table~\ref{tab:impl:calibration}. The values are roughly aligned, so I use
the corresponding values for $\alpha$ going forward.

\begin{table}[htb]
\centering
\caption[Noise Calibration Results]{Mine versus~\citeauthor{vivacqua2017low} dead reckoning errors.}
\label{tab:impl:calibration}
\begin{tabular}{@{}lll@{}}
\toprule
Metric                          & My Value           & Value from~\cite{vivacqua2017low} \\ \midrule
Mean Final Positional Error     & 2.06m              & 2.4m                                                          \\
Max Final Positional Error      & 5.85m              & 5.3m                                                          \\
Angular Error Accumulation Rate & 0.25 $\degree$/min & $\sim$1$\degree$/min                                         
\end{tabular}
\end{table}


\section{Objective Functions and Optimization}

This work models camera placement as an optimization problem, 
so the objective function needs to be defined clearly.

Past work has used a wide range of metrics for landmark placement, ranging from
maximally reducing the trace of the robot's covariance matrix (an upper bound on location
uncertainty)~\ref{beinhofer2013robust}, to entropy of the final
robot state~\ref{beinhofer2013effective}, to the mutual information between
robot states and observations~\ref{beinhofer2011near}. Surveillance literature
has also maximized the observability of a path, essentially defined as
previously unseen path length in the camera's pixel plane~\cite{bodor2007optimal}.

The core difficulty of the multiple-camera placement problem is the exponential
growth of possible placements. In fact, most variants of the problem are shown to be NP-hard~\cite{beinhofer2014landmark}.

The authors in~\cite{beinhofer2011near} deal with the NP-hardness 
using \textit{submodularity}, which is a diminishing returns property.
If the objective function can be proved submodular, the greedy
algorithm is guaranteed find a value within 63\% of the global optimum.
Beinhofer et al. show that their formulation of mutual information is submodular --
I use this measure as my objective function. For comparison, I also
evaluate the entropy of the final robot state, and the mean
value of the trace of the covariance matrix.

However, evaluating any of the objective functions even once is expensive, which is why authors have generally
only optimized one or two camera settings at a time. Since I aim
to consider two orientational degrees of freedom, as well as field of view of the cameras, 
I require a means of cutting down the search space. In Section~\ref{cameraplacement:cheap} 
I present a new objective function which captures the localization power of a single camera placement.
The best results from this metric are used as parameters
for the expensive greedy optimization.

***TODO rewrite this it's ugly**

\subsection{Entropy and Mutual Information}
Two of the possible objective functions utilize entropy, which can
be seen as a measure of uncertainty of a random variable. 
The entropy of a discrete random variable $h(X)$ is maximized when
every outcome is equally probably (a ``flat'' distribution).
It is minimized when a specific outcome contains all of the probability.

In mathematical terms, entropy of a discrete random variable $X$ taking on values
${x_1, x_2..., x_n}$ can be written as:

\begin{flalign}
    h(X) &= -\sum_{i}^{n} p(X = x_i) log(p(X = x_i))
\end{flalign}

Differential entropy extends the standard definition to continuous probability 
distributions:
\begin{flalign}
    h(X) &= -\int p(X = x) log(p(X=x)) dx
\end{flalign}

Joint entropy of a set of continuous random variables $X_1, X_2...X_n$ is defined as
\begin{flalign}
\notag    h(X_1,...X_n) &=  \\
                  &   -\int p(X_1 = x_1,...X_n=x_n) logp(X_1=x_1,...X_n=x_n)d(x_1,...x_n)
\end{flalign}

Conditional entropy of random variable X, given a random variable Y is calculated as
\begin{flalign}
    h(X|Y) &= h(X,Y) - h(X) \\
           &= -\int \int p(X=x|Y=y)log(p(X=x|Y=y))dx p(Y=y) dy \\
           &= \int h(X|Y=y) p(Y=y) dy
\end{flalign}
 
Finally, the mutual information between two random variables $I(X;Y)$ is
defined as
\begin{flalign}
    I(X;Y) &= h(X) - h(X|Y) \\
           &= h(Y) - h(Y|X)
\end{flalign}

Notice that this can be interpreted as the reduction in uncertainty of $X$, 
once $Y$ is known. Or in other words, it is a measure of how
informative $Y$ is about $X$.

I will refer to these equations in Section~\ref{sec:cameraplacement:mutualinformation}.



\chapter{Camera Model}
\label{chap:cameramodel}

The analysis presented in the Chapter~\ref{chap:cameraplacement} is based on the navigation of an
autonomous vehicle guided only by dead reckoning. Errors accumulate over time need
occasional updates to reduce the positional 
uncertainty of the robot. The sensor selected to provide this
information is the camera, which is already widely deployed
in modern city infrastructure.

The updates to Kalman Filters contain two components: a value (in this case, an $(x,y)$ position),
along with an uncertainty in the form of covariance. This chapter
develops the simulated camera which is used to report vehicle 
location and uncertainty.
 
\section{Localization and Error}

\subsection{Approach}

In a real-world implementation of the proposed system, a camera would wirelessly
transmit estimated vehicle positions and associated uncertainty to vehicles in its field of view.

To enable position estimation, the camera needs to know its own world position and orientation, 
as well as the position and orientation of the ground plane. In in this work,
I assume the ground plane to be flat (a locally reasonable assumption, especially along roads), 
and equal to the $Z = 0$ plane. I report the position $(x,y)$ of the vehicle
as the estimated geometric center of the vehicle on the ground plane. 
Note that in practice, using the midpoint of the front or rear axle, rather than the center, 
may be a more consistent reference between vehicles.

Once the computer vision algorithm running on the camera estimates the center of the vehicle,
we must derive an uncertainty in the form of a covariance. I develop a mathematical model of localization error,
and analyze its properties. 

To start with, I develop a circular error radius, $r$, which I show to contain
the true error 99\% of the time. While circular error radii are more amenable to analysis and are good
estimates at steeper pitch angles, error ellipses are more realistic at low pitch angles and are an 
even more accurate error bound. Therefore, I also transform the circular error bounds into error ellipses at the end of this chapter.

Both circles and ellipses can be encoded as three standard deviations of a normal distribution
using the covariance matrix for the estimated state $(x,y)$. The error radius directly determines
the operational capability of the autonomous vehicles, thus must be accurately represented.

I model three components that contribute to the error radius:
\begin{enumerate}
    \item Error in the camera's own location and orientation due to imprecise installation
    \item The algorithmic error in the estimation of the pixel that represents the estimated center of the vehicle
    \item The ground area covered by the pixel that represents the estimated center of the vehicle (encoding camera's finite precision)
\end{enumerate}
 
These will be analysed in turn. Note that I assume calibrated cameras without distortions, and no motion blur.


\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/a_estimate_example.pgf}
    \end{center}
    \caption[Example Predictions about a World Point]{The red point is a true world location, while green points were
    estimated by a large sample of cameras. Discrepancies occur due to the three errors listed above. We must associate an error with each green point.}
    \label{fig:camera:dist}
\end{figure}



\subsection{Location and Orientation Error}

Of the three sources of error, this one is the most difficult to reason about. 
Many transport authorities have standard mounting specifications, including mount heights, for cameras~\cite{StreetscapeGuideance}.
However, none of these specify the tolerances in location and orientation, and we will have to make some educated guesses.

I assume for now that the position and orientation error is normally distributed, with
95\% of positions within 0.03 meters in any direction, 
and orientation within a 0.5 degree cone about the principal axis 95\% of the time.
In mathematical terms, this means $\Delta_{x}$, $\Delta_{y}$, and $\Delta_{z}$ 
are all sampled from $\mathcal{N}(0, (0.03/2)^2)$. The orientation error is given as
$\Delta_{orientation} \sim \mathcal{N}(0, (0.5/2)\degree2)$.
These tight bounds could be achieved using some sort of post-installation 
calibration procedure. 

% TODO discuss such a calibration procedure!

%Orientation accuracy can be further improved by adding 
%an interial measurement unit to the camera, which on its own
%could provide better than $0.5\degree$ accuracy in the roll and raw directions according
%to~\cite{kok2017}.

To translate these values into an uncertainty on the ground plane, consider that
any horizontal camera movement ($\Delta_{x}$, $\Delta_{y}$) from its true position simply translates
the predicted vehicle location and therefore increases its error by the length of the translation.
I call this combined value $err_{xy}$ and at two standard deviations it is bounded above by
$\abs{err_{xy}} = 0.03$.

\begin{figure}[htb]
\centering
\begin{subfigure}[b]{.45\textwidth}
  \centering
  \resizebox{\linewidth}{!}{\input{figures/camera/xyz_error.pstex_t}} 
  \caption{Positional Errors}
  \label{fig:camera:xyz error}
\end{subfigure}%
\begin{subfigure}[b]{.45\textwidth}
  \centering
  \resizebox{\linewidth}{!}{\input{figures/camera/orient_error.pstex_t}}
  \caption{Orientational Error}
  \label{fig:camera:orient error}
\end{subfigure}
\label{fig:camera:errors}
\end{figure}
Vertical translation, $\Delta_z\sim \mathcal{N}(0, (0.03/2)^2)$
of the camera induces an error as $err_{z}(d, h) = \Delta_z \frac{d}{h}$ %, with variance $\sigma_{z}(d,h)^2 = (d/h)^2(0.03/2)^2$ 
where $d$ is the ground distance to the vehicle (on the XY plane), and $h$ is the true intended
height of the camera.

Thus, the total contribution from the camera's positional error to the error radius
will (95\% of the time) be bounded by $(err_{xy} + err_{z}(d, h))$, which evaluates to

\[ err_{xyz}(h, d) = 0.03 + \frac{0.03d}{h} \]

Similarly, it can be shown that using a conical error region about the desired principal axis
of magnitude $\Delta_{orientation} \sim \mathcal{N}(0,(0.5/2)\degree)$, 
for a camera at height $h$, and ground distance to the vehicle $d$, the
error induced in the estimation of the vehicle's center is 

\[ err_{orientation}(h, d) = h\tan(\tan^{-1}(d/h) + \Delta_{orientation}) - d \] %\frac{hd + h^2\Delta_{orientation}}{h - d\Delta_{orientation}} - d$.




\subsection{Algorithmic Error}

The vehicle localization algorithm needs to scan the pixel plane for vehicles,
and return pixel representing the center of vehicles on the ground plane.
The center of the pixel projected onto the ground plane is taken as the vehicle center.

This process can be arbitrarily complex. I assume that at the time of deployment, 
computer vision algorithms are very good at this task, but may still
be wrong by several pixels. I analyze the effect of this inaccuracy
in the model validation Section~\ref{sec:camera:validation} and refer to the Euclidean distance
in the pixel plane from the true center pixel to the calculated one
as $\Delta_{alg} \sim \mathcal{N}(0, \eta_{alg}/2)$.

\subsection{Error from Camera Limitations}

Lastly, the error induced by the camera must be accounted for. Consider that 
in the limit, a vehicle is covered by exactly one pixel, meaning any estimate
of its position is at least as uncertain as the ground area 
covered by that pixel. On the other hand, if a vehicle
exactly fills the camera plane and the localization algorithm perfectly determines
the center of the vehicle in the pixel plane, the estimate still cannot
be better than the area covered by the single pixel.

I call the area covered by a pixel $p$, $A(p)$. $A(p)$ is also a function of the camera
position and orientation, and the camera parameters (field of view and resolution),
but these are omitted here as they are constant for any given camera.

Correspondingly, an approximate upper bound on the longest distance
within a pixel can be calculated as the diagonal of a square,
resulting in a ground distance of $err_{pixel}(p) = \sqrt{(2A(p))}$. 


\subsection{Complete Error Model}
\label{sec:cameramodel:r}

I now combine the three sources of error to determine an approximate
error radius for the vehicle localization.

\[
    r(h,d,p) = err_{xyz}(h,d) + err_{orientation}(h,d) + \Delta_{alg}*err_{pixel}(p)
\]

For any individual camera, the $err_{xyz}$ and $err_{orientation}$ terms introduce
a directed bias for any point in the pixel plane, though over many installed cameras
these terms have a normal distribution. The $\Delta_{alg}$ is 
modeled as normal as well. To take advantage of this, I also multiply each with a tuning
parameter, related to their variances, which I use to trade off error bound size versus
error bound accuracy. The $err_{pixel}$ term is a systematic 
and complex function of camera parameters and estimated vehicle location, and
the only component with a non-zero mean over a large number of samples.

To use $r$ in simulation, when a vehicle is visible to a camera,
I project the ground truth vehicle center to a (slightly-mispositioned) camera, which
automatically incorporates the positional and orientational errors.
The algorithmic error shifts the incident pixel a distance sampled according to the distribution 
of $\Delta_{alg}$, in a randomly sampled direction. 
Finally, I use a second, fake, perfectly positioned camera 
to calculate the pixel error and reproject into a world position from the perturbed pixel.

\section{Error Model Validation}
\label{sec:camera:validation}

In this section I evaluate the error radius function developed above. It is a good error model if,

\begin{enumerate}
    \item For a given estimated vehicle location, the true location should be within the predicted
          error radius most of the time. My formulation aims for 95\% accuracy.
    \item It is not over-conservative: the tighter the bound, the more useful the update is
          for vehicle navigation.
\end{enumerate}

I also examine how the different components that make up the error radius vary
as the pitch of the camera varies, followed by a brief look at the
effect of algorithmic inaccuracy. Experiments were performed
with the camera placed at a height of 6 meters above the ground, and a fixed
field of view of 60 degrees horizontally and vertically. Rotation about the Z axis
was ignored as it does not affect results.

\subsection{Quality of $r$ as an Error Bound}

For this and the following subsection I assume a perfect vision algorithm,
setting $\Delta_{alg}$ to 1.

\begin{table}[htb]
    \centering
    \caption[$r$ as an Error Bound]{Success Rate of $r$ as an Error Bound}
    \label{tab:camera:bound accuracy}
    \begin{tabular}{@{}ccc@{}}
        \toprule
        Total Samples & \# Within Error Bound & \% Within Error Bound \\ \midrule
        125000              & 124290                  & 99.432 
    \end{tabular}
\end{table}

Table~\ref{tab:camera:bound accuracy} shows that more than 99\% of 125000 sampled camera positions
and vehicle positions, the distance from the predicted vehicle position to 
its actual position is within the calculated error radius. This
is actually better than intended, since the construction was for 95\% accuracy.

It is easy to achieve an error bound that is always valid: simply set it very high.
Figure \ref{fig:camera:diff bound error} shows two important characteristics. Firstly, the difference between
the predicted error radius and the actual error distance is small, meaning
it is not an excessively high bound. Secondly, when the bound is \textit{incorrect} (very small region less than zero in blue)
the true error only slightly exceeds the error bound.

\begin{figure}[htb]
    \begin{center}
        %\input{figures/camera/diff_radius_true_error.pgf}
        \includegraphics{figures/camera/diff_radius_true_error.pdf}
    \end{center}
    \caption[Bound Minus True Distance]{Computed radius minus actual error distance.}
    \label{fig:camera:diff bound error}
\end{figure}

\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/proportion_radius_true_error.pgf}
    \end{center}
    \caption[Bound Minus True Distance as a Proportion]{Radius error bound minus actual error distance as a proportion of radius.}
    \label{fig:camera:proportional diff}
\end{figure}

To look at how tight the error radius computation is, I present the same chart
but plot the residual between predicted radius and actual error distance as a 
proportion of the predicted radius. Figure~\ref{fig:camera:proportional diff} shows that for the majority
of predictions, the error radius is far too large -- most commonly it is 3 times 
too large. 

Note that the true error is dependent on several random variables -- sampled positional and orientational
error, and perturbations modeling algorithmic error (pixel ground area is not randomly sampled). Each
of these are approximately Gaussian and added together r(in absolute values, meaning the computed error bound is intended to be in the tail
of a rectified Gaussian distribution as well. Thus, we expect most samples to
fall far from the error bound, which is exactly what we see.
Attempts to make the bound tighter resulted in a loss of the 99\% accuracy.

These results suggests the $r$ is good at modeling the true error, even though during construction
I did not model interplay between some of the factors (eg. how orientation 
or positional error affects pixel area inaccuracy).

\subsection{Error Radius as a function of Pitch Angle}

The \textit{XY} offset of a camera induces a constant distance on the ground plane. However,
all other components modeled in the error bound are dependent on pitch angle. Notably,
pixel areas diverge as the camera angle approaches horizontal. Figure~\ref{fig:camera:radius versus pitch}
illustrates the components of the error radius as a function of pitch.

\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/mean_error_pitch_variation.pgf}
    \end{center}
    \caption[$r$ as a Function of Pitch Angle]{How $r$ varies as a function pitch angle. Error bars are left out for clarity.}
    \label{fig:camera:radius versus pitch}
\end{figure}


This gives us an intuition for how each factor impacts the radius.
As expected, orientation and pixel area contributions
come to dominate, while at $90\degree$, when the camera is pointing straight down, the dominant
error is the camera's horizontal translation.

Also note that the pixel-area component of the error is the only component that
can be affected by the camera's configuration and the algorithmic 
accuracy in detecting the center pixel of the vehicle. This leads
to some fundamental limits discussed in Section~\ref{sec:camera:implications}.

\subsection{Impact of Algorithmic Inaccuracy}

The prior subsections were all presented without any algorithmic inaccuracies.
However, no computer vision algorithm can be expected to perfectly determine
the pixel containing the center of the vehicle.

\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/algorithm_influence.pgf}
    \end{center}
    \caption[Effect of Algorithmic Inaccuracy]{The effect of algorithmic inaccuracy on the mean value and accuracy of $r$. Error bars indicate two standard deviations.}
    \label{fig:camera:algorithm effect}
\end{figure}


Figure~\ref{fig:camera:algorithm effect} shows how the mean error radius changes as a function of the 
algorithmic error in red. In blue, I show that the accuracy as a bound is minimally influenced 
by the introduction of the extra error source. This implies that my bound of
algorithmic error is indeed well matched to the growth of the actual error.


\section{Limits and Implications}
\label{sec:camera:implications}

Having validated that the error model serves as a good bound on the real errors, we can use the function $r$
to reason about fundamental limits of the proposed system.

\subsection{Effect of Resolution}

One interesting questions to ask is: if we had infinite resolution,
how well could we perform? Alternatively, how much accuracy do we gain from increasing resolution?

\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/resolution_angle_effects_mean.pgf}
    \end{center}
    \caption[Resolution Limits]{How increased resolution can increase the error bound at different pitch angles.}
    \label{fig:camera:resolution}
\end{figure}

Figure~\ref{fig:camera:resolution} shows that the mean error bound stops decreasing
significantly beyond a resolution of 2000 by 2000 pixels. A smaller pitch 
angle gains further from higher resolution, which is in line with what we 
expect from Figure~\ref{fig:camera:radius versus pitch} where we saw that more horizontal cameras
have a larger part of their error bound stemming from pixel area uncertainty,
which can in turn be reduced further by increasing resolution.

A similar behavior can be had by decreasing the field of view, which was for
these experiments set to $60\degree$ horizontally and vertically. 
In the next chapter, I fix the resolution and only optimize for the field of view,
since a higher resolution always increases the performance
of the proposed system at no cost and is thus not interesting.

\subsection{Installation Requirements}

Figure~\ref{fig:camera:resolution} was created assuming the following 95\% bounds: positional error
within 3 centimeters in any direction, orientational error within a
$5\degree$ diameter cone about the intended axis, 
and an algorithmic inaccuracy radius of 4 pixels. Table \ref{tab:camera:best error bounds}
shows how well such a configuration might perform at different pitch angles (sampled from
a large number of cameras instantiated according to the allowed variations,
and across various world vehicle positions). 

% TODO talk about sufficient accuracy required for navigation!!

%Angle: 90, Mean radius across resolutions: [0.40197440201387813, 0.19293124638700396, 0.15081008417570249, 0.11928250253476221, 0.10350244344841893, 0.094265799817414259, 0.09135945934363611]
%Angle: 70, Mean radius across resolutions: [0.45010524499283522, 0.21527710378253404, 0.16885180262701374, 0.13411132016861682, 0.11625843106774317, 0.10492112594687218, 0.10059377863697491]
%Angle: 50, Mean radius across resolutions: [0.70077300652909458, 0.33235179394953457, 0.26343141230548817, 0.20505057188734008, 0.1824444301888852, 0.16520421425770482, 0.15808103541200413]
%Angle: 40, Mean radius across resolutions: [1.19613352368245, 0.57308821388154529, 0.44891176913717401, 0.35664516588458312, 0.31671915528379335, 0.28935555994940343, 0.29652864029514137]
%Angle: 90, Median radius across resolutions: [0.4021706919463145, 0.19317966321015673, 0.15129084585686103, 0.11946773675605801, 0.10354271824198089, 0.094174543733546487, 0.091384413508090304]
%Angle: 70, Median radius across resolutions: [0.43152828692038625, 0.20319800518188907, 0.15991538828760138, 0.12556952323628817, 0.10753734230030643, 0.097301716687867551, 0.094192860259294769]
%Angle: 50, Median radius across resolutions: [0.5812734705835072, 0.27710777940867815, 0.20624173044352512, 0.16315340157784369, 0.14536687331250048, 0.13022587786212858, 0.12444617233290693]
%Angle: 40, Median radius across resolutions: [0.74747875637696892, 0.36061541520381596, 0.27549923065859278, 0.21482443305607002, 0.18843698803004749, 0.16809772152143937, 0.17067692688862446]
%Angle: 90, 99th percentile radius across resolutions: [0.42027722082110963, 0.20973881691227797, 0.16802942748374572, 0.13753351941756201, 0.12153702404932258, 0.11220089853368999, 0.1092350371485367]
%Angle: 70, 99th percentile radius across resolutions: [0.64096122009177803, 0.31288726892431312, 0.24971344961445791, 0.20086634699326877, 0.17589238199538776, 0.16510546392126713, 0.16068880262436383]
%Angle: 50, 99th percentile radius across resolutions: [1.6482785424932636, 0.78631792753850227, 0.64515988844698235, 0.51926067759233085, 0.44866023607367178, 0.42721999031097868, 0.40268379296792561]
%Angle: 40, 99th percentile radius across resolutions: [4.6372525003137337, 2.3852774525327942, 1.9460014599579245, 1.6544023827949428, 1.3719493458703107, 1.3728843421688703, 1.3448926870675881]


\begin{table}[htb]
    \centering
    \caption{Sampled Prediction Error Bounds using 2000x2000 resolution}
    \label{tab:camera:best error bounds}
    \begin{tabular}{@{}ccccc@{}}
        \toprule 
        \textbf{Pitch}       & \textbf{Mean (m)}   & \textbf{Median (m)} & \textbf{$95^{th}$ Percentile (m)} &  \\ \midrule 
        $90\degree$ & \textbf{0.1035} & \textbf{0.1035} & \textbf{0.1172}           &  \\ 
        $70\degree$ & \textbf{0.1163} & \textbf{0.1075} & \textbf{0.1658}           &  \\
        $50\degree$ & \textbf{0.1824} & \textbf{0.1454} & 0.3866           &  \\
        $40\degree$ & 0.3167 & \textbf{0.1884} & 1.022            &  \\ \midrule
    \end{tabular}
\end{table}


As discussed in Section~\ref{sec:intro:motivation}, allowable lateral deviations are generally
quoted at $\pm$10-25cm. The table above shows in bold which camera placements would be 
within these limits. Note that even though the mean and 95\% percentile error bounds
at lower pitch angles are high, the median tells us that most measurements
from such a camera would still be useful.

Lastly, I present some installation accuracies that would need to be achieved
in order for the system to provide acceptable performance. In bold in 
Tables~\ref{tab:camera:best error bounds}, ~\ref{tab:camera:70deg}, and
~\ref{tab:camera:45deg} I highlight positional and orientational 
constraints that provide sub-25 centimeter median localization performance.



\begin{table}[htb]
\centering
\caption{Median Localization Bound for 70$\degree$ pitch, 2000x2000 pixels}
\label{tab:camera:70deg}

\begin{tabular}{ccccccccc}
\toprule
& \multicolumn{8}{c}{\textbf{Orientation Error, 95\%}} \\
\cmidrule(ll){2-9}
\multicolumn{1}{N}{\textbf{95\% Pos. Error (m)}} & 0$\degree$& 0.2$\degree$& 0.4$\degree$ & 0.6$\degree$ & 0.8$\degree$ & 1$\degree$ & 1.2$\degree$ & 1.4$\degree$\\
\cmidrule(lr){1-1}
\cmidrule(ll){2-9}
0.0 & \textbf{0.017} & \textbf{0.034} & \textbf{0.051} & \textbf{0.069} & \textbf{0.087} & \textbf{0.103} & \textbf{0.124} & \textbf{0.140} \\
0.1 & \textbf{0.177} & \textbf{0.196} & \textbf{0.214} & \textbf{0.230} & \textbf{0.248} & 0.268 & 0.280 & 0.298 \\
0.2 & 0.342 & 0.360 & 0.378 & 0.394 & 0.409 & 0.430 & 0.443 & 0.460  \\
0.2 & 0.503 & 0.524 & 0.538 & 0.552 & 0.566 & 0.586 & 0.605 & 0.616  \\
0.4 & 0.665 & 0.685 & 0.702 & 0.715 & 0.737 & 0.757 & 0.765 & 0.794  \\
\end{tabular} 
\end{table}


\begin{table}[htb]
\centering
\caption{Median 99\% Localization Bound for 45$\degree$ pitch, 2000x2000 pixels}
\label{tab:camera:45deg}

\begin{tabular}{ccccccccc}
\toprule
& \multicolumn{8}{c}{\textbf{Orientation Error, 95\%}} \\
\cmidrule(ll){2-9}
\multicolumn{1}{N}{\textbf{95\% Pos. Error (m)}} & 0$\degree$& 0.2$\degree$& 0.4$\degree$ & 0.6$\degree$ & 0.8$\degree$ & 1$\degree$ & 1.2$\degree$ & 1.4$\degree$\\
\cmidrule(lr){1-1}
\cmidrule(ll){2-9}
0 & \textbf{0.027} & \textbf{0.060} & \textbf{0.083} & \textbf{0.114} & \textbf{0.145} &\textbf{ 0}.180 & \textbf{0.211} & \textbf{0.228} \\
0.1 & \textbf{0.222} & \textbf{0.250} & 0.285 & 0.324 & 0.338 & 0.365 & 0.398 & 0.446 \\
0.2 & 0.421 & 0.460 & 0.484 & 0.504 & 0.540 & 0.561 & 0.599 & 0.636 \\
0.3 & 0.624 & 0.661 & 0.682 & 0.713 & 0.761 & 0.764 & 0.796 & 0.837 \\
0.4 & 0.839 & 0.828 & 0.873 & 0.921 & 0.950 & 0.971 & 1.030 & 1.030 \\
\end{tabular} 
\end{table}

These tables show that achieving a 10 centimeter installation accuracy 95\% of the time is an absolute necessity, while angular deviations of up to
one degree are acceptable. Finally, Table~\ref{tab:camera:alg} shows how algorithmic inaccuracy impacts localization:
with the usual 3 centimeter placement and a 0.5 degree orientation error as well as 45 degree pitch the algorithm must be able to
localize the vehicle center to within 20 pixels at the most. 

\begin{table}[htb]
\centering
\caption{Median Localization Bound for 45$\degree$ pitch, 2000x2000 pixels with increasing algorithmic errors}
\label{tab:camera:alg}
\begin{tabular}{ccccccccc}
\toprule
Error (pixels) & 4 & 8 & 12 & 16 & 20 & 24 & 28 \\ \midrule
Median $r$ (m)	&  \textbf{0.157} & \textbf{0.183} & \textbf{0.219} & \textbf{0.237} & 0.260 & 0.294 & 0.313 \\
\end{tabular} 
\end{table}

\section{Error Circles to Error Ellipses}

The prior sections used circular error bounds, which while also accurate and insightful,
can be slightly improved upon. A quick inspection of position estimates versus their true
positions in Figure~\ref{fig:camera:distributions} shows that at higher pitch angles, the estimates lie 
along an oval rather than a circle.

\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/actual_position_estimate_distributions.pgf}
    \end{center}
    \caption[Distribution of Estimates]{Ground plane estimates (green) of true position (red) across a large sample of cameras showing algorithmic, positional, and orientational error.}
    \label{fig:camera:distributions}
\end{figure}

I thus reshape the error circles into ovals of equal area, with the major
and minor axes determined by the ratio of ground height to width of the chosen pixel.
The orientation of the ellipse is given by the angle from the camera to the pixel's
ground position.


\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/ovals_vs_circles.pgf}
    \end{center}
    \caption[elliptical versus circular error]{An example of where elliptical error bounds are more accurate than circular ones. Red circles represent error circles, while error ellipses are in blue. Note the vast majority of both are accurate bounds (very lightly drawn ovals and circles)}
    \label{fig:camera:oval better}
\end{figure}

This modification improves the accuracy of the error bound even further, as shown
in Table~\ref{tab:camera:ellipse vs circle}. A case of this occuring is shown
in Figure~\ref{fig:camera:oval better} which samples many camera positions and orientations
estimating the location of the point in red. The circular and elliptical 
error bounds that contain the red point are shown faintly in the background. One
case where the circular error bound fails while the elliptical one succeeds
is shown in bold.

\begin{table}[htb]
    \centering
    \caption[Elliptical versus Circular Error Bound]{$r$ as an Elliptical versus Circular Error Bound}
    \label{tab:camera:ellipse vs circle}
    \begin{tabular}{@{}llll@{}}
        \toprule
        Bound Type    & \# Samples  & \# Within Error& \% Within Error \\ \midrule
        Circular      &   125000              & 124290                  & 99.432  \\
        Elliptical    &   125000              & 124528                  & \textbf{99.622}
    \end{tabular}
\end{table}


In simulation I use oval error bounds as these more effectively represent the 
actual shape of errors, encoding the angle $\phi$ and lengths of major axis $M$
and minor axis $N$ into the covariance between $x$ and $y$.

\[
\Sigma_{x,y} =
  \begin{bmatrix}
    M^2 \cos(\phi)^2 + N^2 \sin(\phi) &  (M^2 - N^2) \sin(\phi)^2 \cos(\phi)^2 \\
    (M^2 - N^2) \sin(\phi)^2 \cos(\phi)^2  &  M^2 \sin(\phi)^2 + N^2\cos(\phi) 
  \end{bmatrix}
\]

\subsection{Practical Issues}
In practice, I found that due to timing delays between parts of the simulation,
the camera update delivered to the vehicle was slightly old compared to the
vehicle's actual state (much as it might be in a real implementation). This
difference ended up having a big impact on the EKF update step, and even
more so if an oval was used (when only 1 dimension was stale, the stale
value would pollute the remaining good value).

To fix this, I did two things: firstly, I included a timestamp in the 
camera's update message, which enabled me to apply a time difference
correction based on the vehicle's instantaneous velocity. This is 
similar to early approaches proposed for handling delayed measurements
such as those in~\cite{larsen1998incorporation}\cite{tessier2006real}.
To further avoid partially stale updates from corrupting the vehicle's state,
I also made the oval 50\% more circular, as defined by the ratio
of its major to minor axis lengths. Together these proved to 
provide adequate performance. The issue of timing delays is not considered 
further, though would need to be re-examined in a physical implementation.
 
\section{Summary}

This chapter has developed and evaluated the camera sensor model
used in the following chapter to provide correctional updates
to an autonomous vehicle. To achieve this, the center
of the vehicle needs to be estimated in world coordinates. This
is associated with an error, which informs how trustworthy
an individual update is.

The error bound developed here, $r$, is a function of camera height,
camera pixel areas, and the horizontal distance to the vehicle center.
It was shown to be an effective upper bound, 99\% of the time, and
small enough to be usable in a real-world implementation (assuming
the installation bounds can be achieved). The effect of pitch angle
was extensively analyzed, and Section~\ref{sec:camera:implications}
explored the impact of resolution in the limit, providing a 
fundamental limit on the effectiveness of the proposed system,
as well as some real-world requirements in terms of
installation and algorithmic accuracy. Finally, I showed
the effectiveness of reshaping circular bounds into elliptical ones.


\chapter{Camera Placement}
\label{chap:cameraplacement}

Two of the questions this dissertation sets out to answer
are related to the placement of cameras for localization and navigation.
Normally, problems of this type are formulated one of two ways. Either,
given a budget of $n$ cameras, find the best placement according to
some objective function, or given some objective function criteria,
place as many cameras as required. I focus on the first formulation
in this chapter.

I split the results in this chapter based on the objective
function optimized.

\section{A Cheap Objective Function}
\label{cameraplacement:cheap}

Since I hope to explore up to four degrees of freedom in a single camera placement (
pitch, yaw, horizontal and vertical field of view), the search space grows even
faster than what previous authors in this field have dealt with. Additionally,
the mutual information objective function used in the following section is an extremely expensive to compute.
Due to its cost and the size of the search space, I develop a cheaper objective that can be used for optimizing
the parameters of a single camera. The interesting points from this search are then used
as discretized points in the full optimization.


\subsection{Objective Function for Localization}

The task of a camera is to maximally aid in vehicle localization: a natural
objective is therefore to maximize the reduction in vehicle uncertainty
after an update. This intuitive means we have to provide the best possible
updates to the vehicles, or minimize $r$ the error radius, in all possible updates sent to vehicles
passing by the camera. I consider circular errors here,
since they neatly quantify uncertainty as a single number.

To quantify ``all possible updates'', consider that we know roughly where vehicles will be: 
for vehicles to be running they must self-localize to at most $\pm25cm$ lateral error. 
Additionally, if the controllers are implemented well, their position estimate should be exactly on the path
to be traversed. Thus, assuming the 25cm represents two standard deviations,
the vehicle will be within 25cm of the path 95\% of the time.

Using this information, I place a normal distribution
along the path, which defines the probability of being at a distance
from the center line at all points, and call it $p(x,y)$.

Now, I can formulate the objective function as minimizing the
expected reported error radius over the camera's pixel plane.
In other words, for a camera $Cam$ 
\begin{flalign}
    obj_1(Cam) &= \sum_{(i,j)\in Cam}p(proj_{Cam}(i,j)) * r_{Cam}(i,j) 
\end{flalign}

where $(i,j)$ represent the pixels in the camera plane, $proj_{Cam}$ projects
a pixel onto the ground plane from the camera, and $r_{Cam}(i,j)$ is shorthand for 
the error radius for a prediction at pixel $(i,j)$ as given in Section~\ref{sec:cameramodel:r}.

The problem with this formulation is that when allowing the camera
to minimize the objective, it naturally selects views
where the road is not visible at all, driving the expectation to 0.
Really, the problem needs to be reformulated as a maximization to obtain
sensible results. To do this, $r$ needs to be inverted, but there are 
many ways to achieve this.

My insight is to utilize the Kalman gain, $\bm{K}$, from Section~\ref{chap:impl:vehicle:ekf}.
It directly quantifies how much the vehicle's localization will ``trust'' an update,
and is highest when the error in the measurement is low. This provides
a principled way of inverting $r$.

A simple one dimensional Kalman Filter computes the gain $K = \frac{\sigma^2}{\sigma^2 + \sigma_{measurement}^2}$.
So, I set $\sigma=1$ and $\sigma_{measurement} = (r/3)$, resulting in the following objective function:

\begin{flalign}
    obj_2(Cam) &= \sum_{(i,j)\in Cam}\frac{p(proj_{Cam}(i,j))}{1 + (r_{Cam}(i,j)/3)^2}
\end{flalign}


\subsection{Optimizing Pitch and Yaw with varying Curvature}

I perform an exhaustive grid search over the pitch and yaw parameters using the defined
objective function. The results are presented in Figure~\ref{fig:simpleobjective}.

\begin{figure}[h!]
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/simple_objective/pitch_yaw_0_curvature_annotated.png}
    \label{fig:simpleobjective:0curvature}
    \caption{0 Curvature Scores}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/simple_objective/0_curvature_bestpos_small.png}
    \caption{Configuration \#1, top score}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/simple_objective/0_curvature_pos2_small.png}
    \caption{Configuration \#2}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/simple_objective/0_curvature_pos3_small.png}
    \caption{Configuration \#3}
\end{subfigure}
\hrule
\hrule
\hrule

\begin{subfigure}[b]{0.42\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/simple_objective/pitch_yaw_at_radius_20_annotated.png}
    \label{fig:simpleobjective:0curvature}
    \caption{20m Radius}
\end{subfigure}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/simple_objective/radius_20m_pos1_small.png}
    \caption{Configuration \#1}
\end{subfigure}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/simple_objective/radius_20m_pos2_small.png}
    \caption{Configuration \#2}
\end{subfigure}
\hrule
\hrule
\hrule
\begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/simple_objective/pitch_yaw_at_radius_5_annotated.png}
    \label{fig:simpleobjective:20radius}
    \caption{5m Radius}
\end{subfigure}
\begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/simple_objective/radius_5m_pos1_small.png}
    \caption{Configuration \#1}
\end{subfigure}
\caption[Cheaper Objective Function Results]{Results and high scoring orientations from the cheaper objective function.}
\label{fig:simpleobjective}
\end{figure}

The first images in a row show the evaluated objective function, the others
are diagrams of the path and camera's field of view. The displayed
configurations the top-scoring and distinct orientations (I ignored
symmetric cases). The filled gray areas are world obstacles
that I place to imitate walls.

The presented optimal points match what one might intuitively expect 
for camera placement: ideally, we maximize both the length of the path
we can spot at once, have the path trace through the highest
pixel density part of the field of view -- this gives the lowest
error during location prediction.

As the curvature increases, the number of different ``good'' 
orientations decreases -- with a highly curved path, there is only
one best choice - the one observing the most of the path possible.

Going forward into the more expensive objectives in Section REF TODO, I use
the depicted configurations as the only allowed orientations to restrict
the search space as much as possible.

%\subsection{Aside: Intersection}
%*** optional -- examine if time remaining at the end ***
%Because we can in this case! ... may be a pain to implement in current code of piecewise paths...


\subsection{Optimizing Pitch, Yaw, and Horizontal and Vertical FoV Simultaneously}

As mentioned in the introduction of this dissertation, I also attempt
to optimize the field of view simultaneously with the pitch and yaw
directions, which has not been examined as a variable before.

To examine this I used a hill-climbing optimization algorithm~\cite{russel2016artificial}
with random restarts to explore the space of options. The optimization
takes steps in each parameter that increase the objective function, effectively
zig-zagging along axes towards (local) maxima.

\subsubsection{Hillclimbing -- Straight Road}

For straight roads, all of the top scoring configurations 
shrunk the field of view as far as was allowed, and oriented themselves to
view the road along the diagonal of the field of view. This makes sense -- 
when observing a straight line, there is no point wasting pixels
on unlikely regions. Additionally, shrinking the field
of view is akin to increasing the resolution, and decreases
the error of predictions. 

Scoring significantly worse are more a few diversified choices,
as depicted in red and purple in Figure~\ref{fig:simpleobjective:hillclimbing straight}.

\begin{table}[htb]
    \centering
    \caption[Hillclimbing Top Scorers]{Three top scoring, diverse results from hillclimbing across four parameters on straight roads.}
    \label{tab:simpleobjective:hillclimb}
    \begin{tabular}{@{}lllll@{}}
        \toprule
        Objective Score & Pitch & Yaw   & H. FoV & V FoV. \\ \midrule
        8.63 (green)            & 57 & $90\pm43$ & 20.1  & 20.1  \\
        ... \\
        6.6787 (red)          & 68 & 90 & 47.2  & 22.9 \\
        ... \\
        3.0710 (purple)          & 39.3 & 90 & 25.1  & 53.5  \\ 
        \bottomrule
    \end{tabular}
\end{table}


\begin{figure}[htb]
    \centering
    \includegraphics[width=3in]{figures/simple_objective/top_hillclimb.png}
    \caption[Top Hillclimbing Result]{Visualization of Top Hillclimbing Result for straight roads.}
    \label{fig:simpleobjective:hillclimbing straight}
\end{figure}

\subsubsection{Hillclimbing -- Curved Roads}

We can obtain slightly more consistently varied results when optimizing
on curved roads. Although the top two scoring parameters
still minimized the field of view, the next best result
had a wide but narrow field of view. Compared to straight roads,
we see more diverse solutions with high scores.

\begin{table}[htb]
    \centering
    \caption[Hillclimbing Top Scorers]{Top 3 results from hillclimbing across four parameters with curvature 0.05.}
    \label{tab:simpleobjective:hillclimb}
    \begin{tabular}{@{}lllll@{}}
        \toprule
        Objective Score & Pitch & Yaw        & H. FoV & V FoV. \\ \midrule
        8.58  (green)   & 56.5 & $270\pm25$ & 21.1  & 20.4  \\
        7.98  (red)     & 61.0 & 270     & 98.2  & 22.1  \\ 
        ... \\
        3.72  (purple)  & 43.3  &  $270\pm45.0$ & 38.8   & 50.8   \\ \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htb]
    \centering
    \includegraphics[width=3in]{figures/simple_objective/hillclimb_curvature.png}
    \caption[Top Hillclimbing Result]{Visualization of Top Hillclimbing Result for a curvature of 0.05.}
    \label{fig:simpleobjective:hillclimb result}
\end{figure}

\subsection{Discussion}

The objective function used in this section can be seen as
a measure of the \textit{instantaneous localization power}
of the camera. In other words, if a vehicle appeared suddenly, or
could only be sent a single update, how can the camera best be chosen
to reduce its localization error, on average.

There are several issues with this metric. Biggest of them
is the embedded assumption that only 1 update can be sent.
In reality, a vehicle spends a longer amount of time in the camera's
field of view if the camera can cover more ground area -- 
though the resulting updates sent to the vehicle may be
less accurate. A useful property of the Kalman
filter is that the uncertainty (covariance) in the state
can only shrink as a result of a measurement, so there is some
utility in sending more updates over a wider field of view,
even at reduced accuracy. 

The advantages of the objective function here are twofold:
firstly, it is cheap to compute, and is the reason I could
do the hillclimbing optimization which
required hundreds of evaluations of the objective. Secondly,
it can be used independently of specific trajectories; all
other objective functions in this work are centered about
a single path. The latter property is discussed 
as a future research direction at the end of this report.

Since I am essentially able to perform exhaustive searches
over the possible pitch and yaw angles, I use this
as a pre-optimization step to discover interesting orientations
to explore further with more expensive objective functions.


% TODO attempt an intersection


\section{Navigation-specific Metrics}

Here I define alternative metrics that I use to
I evaluate camera placements. 

In this section, $x_t$ denotes the robot
state at time $t$, $u_t$ denotes the control command
that drives the odometry motion model from $x_{t-1}$ to
$x_{t}$, and $z^{A}_{t}$ is the measurements received
at time $t$ from cameras in the set $A$. Corresponding
capital letters are used to denote the random variables
for state, controls, a measurements.

\subsection{Entropy of the Final State}
The entropy of a robot's state is a measure of the 
spread of its probability distribution. A peaked distribution
has low entropy -- all the probability is concentrated in a few
regions. A flat distribution is maximally uncertain.

The robot's belief of its own state is estimated by
the EKF running on board. It captures, via the mean state $x_t$
and covariance matrix $\Sigma_t$, the distribution $p(x_t | u_{1:t}, z^{A}_{1:t})$.

At the end of an execution of a path, we can measure the
differential entropy of this distribution as follows:

\begin{flalign}
    h(x_t | u_{1:t}, z^{A}_{1:t}) = \frac{1}{2}log((2 \pi e)^k |\Sigma_t|)
\end{flalign}

Here, $|\Sigma|$ is the determinant of the covariance matrix. Since
entropy is a measure of uncertainty, we try to minimize it via
the camera placements.

It is tempting to try to compute the entropy of the joint robot states
 as the sum of the differential entropy at each time step, ie. 
\begin{flalign}
\notag     h(x_1,...x_t | u_{1:t}, z^{A}_{1:t}) &= \\
h(x_1 | u_1, z^A_1) + ... + h(x_t | u_{1:t}, z^{A}_{1:t})
\end{flalign}

However, this transformation would imply that the state at each time
is completely independent. The actual joint entropy is the value optimized in~\cite{beinhofer2011near},
and is described in the next section.

\subsection{Mutual Information}
\label{sec:cameraplacement:mutualinformation}
I use as my objective functions the one from~\cite{beinhofer2011near}, and
describe it following their notation. The goal is to
maximizing the mutual information between all possible robot states $X_{0:T}$ on a path,
and the observations of the robot along the same path from a set of cameras $A$,
written as $Z^{A}_{1:T}$, given the robot controls $U_{1:T}$.

Formally, this is written as
\begin{flalign}
\notag    I(X_{0:T}; Z^{A}_{0:T} | U_{0:T}) &= 
    h(X_{0:T} | U_{0:T}) - h(X_{0:T} | U_{0:T}, Z^{A}_{0:T}) 
\end{flalign}

The first term is the joint entropy of all possible robot states given
all possible controls. This value is constant for a path. So, maximizing
mutual information is equivalent to minimizing the conditional entropy
of robot states given the observations and controls.

For continuous variables we can compute $h(X_{0:T} | U_{0:T}, Z^{A}_{0:T})$ 
using the following integral (I drop the subscripts for convenience).

\begin{flalign}
    -\int \int p(x | u, z^{A}) log p(x | u, z^{A}) dx p(u, z^{A}) d(u, z^A)
\end{flalign}

Evaluating this integral in closed form is not possible, so I use Monte Carlo
evaluations to sample from the distribution of controls and measurements.

The last missing piece is to obtain the joint distribution of all robot
states $p(x_{0:T} | u^{A}_{1:T}, z^{A}_{1:T})$. This is often
rewritten as:

\begin{flalign}
\notag    p(x_{0:T} | u^{A}_{1:T}, z^{A}_{1:T}) &= \\
    p(x_0) \prod_{t=1}^{T} \eta_t p(z^{A}_t | x_t) p(x_t | x_{t-1}, u_t)
\end{flalign}

This breaks down the joint posterior distribution of all robot states
into the product of the motion model $p(x_t | x_{t-1}, u_t)$ and
the sensor model $p(z^{A}_t | x_t)$ at each time step. I use the sensor model
developed in Chapter~\ref{chap:cameramodel} to obtain
approximate probabilities of getting a concrete sensor measurement,
given the robot's mean positional belief.

Lastly, the $\eta$ normalizer needs to be computed. It can be shown
to be equal to:
\begin{flalign}
    \eta_t = \int p(z^{A}_t | x_t) p(x_t | u_{1:t}, z_{1:t-1}) dx_t
\end{flalign}
the first term is again the sensor model, and the second is
recognizable as the belief of the robot, given by the mean and covariance.
I evaluate the normalizing constant with another small Monte Marlo approximation,
drawing sample states $x_t$ from the robot's positional distribution.

The joint probability distribution can be seen as the probability
that the robot took the exact path that it did on any given
execution of the robot's path. Since these are all drawn
from continuous distributions, technically any such probability
should be zero. I discretize using a small delta when calculating
the probabilities to obtain small but finite values. However,
this also requires that the full Monte Carlo approximation be run
several thousand times to obtain valid results.


\subsection{Mean Trace of Covariance Matrix}
Finally, another measure of uncertainty is the trace of the covariance matrix.
I use the mean trace  across $T$ timesteps in a single trajectory:

\begin{flalign}
    MTrace &= \sum_{t=0}^{T} tr(\Sigma_t)
\end{flalign}

It is interesting to note that the determinant of the covariance,
the trace of the covariance, and the graphical uncertainty ellipse
of the EKF are all related to the eigenvalues of $\Sigma$: the ellipse's major
and minor axes are the eigenvectors scaled by
the square root of the eigenvalues; $tr(\Sigma)$ is the sum of the eigenvalues,
and the determinant is the product of the eigenvalues. Thus,
these measures can interpreted measuring the sum of the radii (squared),
and the approximate area of the ellipse (squared), respectively.



\section{Placing Single Cameras With Navigational Metrics}

\subsection{Straight Roads}



\subsection{Curved Roads}



\section{Multiple Camera Placement}
* Discuss submodularity

* test submodularity by sampling -- make sure minimum overlap of camera ground areas!

* straight roads + 1 curvature

* propose cascaded optimization -- choose possible sensor locations, use cheaper objective to find top 1 or 2 
positions or orientations, add these to the set of possible locations and orientations


\section{Conclusion}

\chapter{Optimizing Road Construction for Navigation}

* brief - look at how to connect two points if various obstacles are put in the way
* maybe try a triangular point an and rectangular block 


\chapter{Summary and Conclusion} 




\appendix
\singlespacing

\printbibliography

\end{document}
