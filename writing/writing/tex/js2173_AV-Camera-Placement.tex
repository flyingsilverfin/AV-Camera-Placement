%% 
%% ACS project dissertation template. 
%% 
%% Currently designed for printing two-sided, but if you prefer to 
%% print single-sided just remove ",twoside,openright" from the 
%% \documentclass[] line below. 
%%
%%

\documentclass[a4paper,12pt,twoside,openright]{report}



\def\authorname{Joshua G. Send\xspace}
\def\authorcollege{Trinity Hall\xspace}
\def\authoremail{js2173@cam.ac.uk}
\def\dissertationtitle{Offboard Camera Placement for Autonomous Robot Navigation}
\def\wordcount{TODO}


\usepackage{epsfig,graphicx,parskip,setspace,tabularx,xspace} 
\usepackage[
    backend=biber,
    style=numeric,
    sorting=ynt
]{biblatex}
\addbibresource{bibliography.bib}

\usepackage{amsmath}
\usepackage{gensymb} 
\usepackage{wrapfig}
\usepackage{physics}
\usepackage{pgfplots}
\pgfplotsset{compat=1.3}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\newcommand{\etal}{\textit{et al.}}
\newcolumntype{N}{>{\centering\arraybackslash}m{.7in}}

%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing

%% START OF MAIN TEXT 

\chapter{Introduction}
\pagenumbering{arabic} 
\setcounter{page}{1} 

\section{Vision}

In the future, private vehicles in urban environments have been made redundant by an efficient
fleet of autonomous vehicles, ferrying passengers on demand between locations in 
the city. The city's pool of vehicles consists of vehicles from various manufacturers, 
but are managed by a public-private partnership, much like a utility, and 
seen as an essential part of the city infrastructure fulfilling the needs of its citizens.

The utility manages not only a large selection of vehicles, but also the infrastructure 
that the autonomous vehicles run on: a network of cameras, placed along roads to 
facilitate vehicle navigation. At this point in time the hardware requirements of
object detection algorithms are miniaturized, and are highly accurate, able to identify
pedestrians and vehicles to within a few centimeters. These locations are
transmitted to nearby vehicles. As a result, GPS, radar, and expensive LIDAR systems
have become largely redundant on the autonomous vehicles.

With this infrastructure, autonomy becomes cheaper to implement,
and more accessible. Vehicle manufacturers work with the utility to keep the cameras 
functional and up to date. City GPS canyons are avoided, and autonomy is enabled 
on everything from bicycles, to small delivery robots, to standard vehicles -- all by 
shifting the sensing task from the individual user to the infrastructure.

\section{Motivation}

The core task of an autonomous vehicle is to navigate correctly and safely from a start
to a destination. This process can be split into major subtasks consisting
of path planning, localization, and safe navigation.

Current autonomous vehicle projects use a large set of sensors, combined with
maps, to solve each of those complex tasks. This comes at a cost: current
Level 3 TODO REFERENCE autonomy attempts for example use LIDAR, which 
provides rich data to perform self-localization and dynamic obstacle avoidance,
but also costs around \$75,000~\cite{lin2018architectural}. The computational
power required to fuse and process data from wheel odometry,
GPS, inertial measurement units (IMUs), cameras, RADAR, LIDAR is also substantial
and implies only large platforms can be used.

I propose moving the localization and navigation sensing tasks
from vehicles to the infrastructure, which is particularly applicable in urban environments.
I examine a static environment containing a single vehicle, with safety and dynamic components
as future research avenues.

Most current localization approaches utilize global satellite navigations systems,
such as GPS and its augmentations like real time kinematic (RTK)~\cite{scherzinger2000precise}.
The US government reports a global 95\% accuracy of 0.715 meters~\cite{USGPSPerformance}. 
However, GPS errors are introduced in urban environments -- according to~\citeauthor{miura2015gps}, even with the
authors' proposed correction steps, the mean localization error was around 5.2 meters in urban canyons. 
In contrast, most authors agree autonomous vehicles can tolerate 0.2 to 0.35 meters lateral error
~\cite{vivacqua2017low}\cite{ziegler2014video}\cite{mattern2010high}.

To achieve these error bounds, current techniques may use computationally heavy visual odometry~\cite{ziegler2014video},
very high precision maps~\cite{mattern2010high}, or expensive LIDAR to localize
lane markings very accurately~\cite{hata2014road}.  

In this exploratory work, I show that if certain installation and algorithmic constraints
can be met, cameras mounted at side of roads can provide the required localization
accuracy without reliance on GPS, LIDAR, or RADAR. Wheel odometry is used to navigate
between updates received from the infrastructure. I then examine the task of 
optimal camera placements and configuration for localization and navigational performance.


\section{Contributions}

The practical questions answered by this work are the following:
%in the context of the vision presented previously, are
%the ones asked by city planners when designing camera placements: 
\begin{enumerate}
    \item How accurate can offboard (ie. in the environment) cameras be for localization tasks?
    \item In which direction should the camera be facing and what should its properties be (for instance, field of view)
          to optimally aid vehicle localization and navigation, and how is related to road curvature?
    \item How should a set of cameras be placed along a path to optimize navigational performance?
\end{enumerate}

The first question relates to feasibility of camera-based localization,
and is addressed at the end of Chapter~\ref{chap:cameramodel}.

The second and third questions examine the optimal camera placement task 
for localization and navigation performance. This is related to past work 
on landmark placement for robotic navigation, where the robots observe landmarks fixed to the environment. 
Here, the use case is inverted to observe the robot as it moves through the environment. 
Thus, it is also closely related to previous work on surveillance networks.

While literature on surveillance using cameras has explored camera placement, none 
has allowed more degrees of freedom than simply adjusting the tilt (pitch) 
and choosing one of a limited set of positions. Further, no exploration of 
properties of cameras has been performed - for instance, the impact
of allowing a larger field of view in the observation task. Lastly, surveillance
tasks normally seek to maximize different objective functions than robotic
navigational performance.

Finally, throughout this work I examine the impact of the environment, specifically
road curvature, on the various tasks presented. To my knowldge, this variable has not been
analysed previously.

In summary, I make the following contributions in this work
\begin{itemize}
    \item An error model for vehicle locazation using infrastructure-mounted cameras, given an installation uncertainty and estimated vehicle position
    \item A feasibility analysis for using cameras for localization, using the developed error model
    \item An analysis of placement of a single camera camera along roads, and the relationship to road curvature
    \item An analysis of placement of sets of cameras along roads, and the relationship to road curvature
\end{itemize}



\section{Overview}

Chapter~\ref{chap:relatedwork} reviews relevant literature in the fields of
localization, landmark placement, and surveillance. Chapter~\ref{chap:impl} discusses required background and the simulation
used for analysis. 

Chapter \ref{chap:cameramodel} develops the error model for cameras
used in simulation, and examines the feasibility of using cameras in the infrastructure for localization.
Chapter ~\ref{chap:cameraplacement} optimizes single and multiple camera placements
in various environments and using different objective functions. This is followed by concluding remarks.

\chapter{Related Work} 
\label{chap:relatedwork}

* Talk about past work with other sensor types, good/bad
talk about possible sensor types. camera, why it's useful
camera versus passive tags versus tech like UWB, pros/cons.

This dissertation touches on a large body of literature ranging from 
control theory to surveillance optimisation. I focus on past work
in surveillance, and... 

* talk about limits of lateral errors allowable for navigation

\section{External Localisation Sensors}
* talk about alternative sensors to cameras here

* discuss how cameras can provide very rich information, including pose, but only focus on (x,y) updates

\section{External Camera-based Localisation}

There is quite a lot of literature here...

\section{Camera and Landmark Placement}

* Beinhofer
* surveillance papers
* camera orientation on vehicles (entropy based paper)

\citeauthor{bodor2007optimal} optimize the surveillance of an area by considering
line segments, representing piecewise paths, on the ground plane from a fixed
height above the ground. The authors consider one variable - pitch,
and during the formulation of their
objective function consider the number of pixels $r$ filled by the target.
This is approximated using the distance to the target and its angle relative 
to the camera (imagine a car rotating in place, shrinking the number
of pixels it takes up).... 

\chapter{Design and Implementation}
\label{chap:impl}

This section outlines the work completed during the development
of this dissertation, and provides mathematical background
referenced to throughout the report.

\section{Approach}

The basis of this work is a simulation developed using industry-standard tools.
The Gazebo~\cite{koenig2004design} robotics simulator is used in conjunction with ROS,
the Robot Operating System~\cite{quigley2009ros}, to model a non-holonomic Ackermann drive vehicle
following predefined paths. A camera is modeled using standard formulations
discussed in Section~\ref{impl:sensors}.

**Todo figure of ackermann drive**

An expensive, high fidelity simulation was used instead of a real hardware
implementation. Simulation allows quickly modifying the environment
and models to examine a greater range of tasks.


\section{Cameras}
\label{impl:sensors}

The standard model for cameras is the pinhole projective model. This model 
keeps lines in the real world as straight lines in the pixel plane,
and can be implemented as a single matrix multiplication

TODO matrices describing world => pixel transformation

My implementation a raytracing module that only uses
translation and rotation portions of the matrices above, which bring the world intp place
in the camera's zero-centered, Z-axis aligned coordinate system. Raytracing
allows adding parametric geometry into the world, which is used later.
Additionally, my formulation can also be used to model non-pinhole camera models,
including ones that better represent ultra-wide field of view cameras. However, this work leaves
examining the impact of such cameras on robot localization and navigation as
a future research direction; I cap the maximum field of view of the pinhol model
at 110 degrees horizontally and vertically, when the perspective model
breaks down~\cite{fleckperspective} (some authors claim different cutoffs all the way down
to 70 degrees~\cite{sharless2010pannini}, but for the analysis presented here it won't 
matter too much). 

An infrastructure-mounted camera's task is to localize vehicles in its pixel plane 
into world coordinates, and provide an error bound. The error bound is developed 
in Chapter~\ref{chap:cameramodel}. To localize the vehicle, the camera needs to
be provided with its own world orientation and location, as well as its parameters.
\textit{Camera parameters} will be used to mean both the horizontal
and vertical field of view of the camera, as well as its resolution. These can be
combined to estimate the geometric center of the vehicle, on the ground plane. Thus,
the update from the camera is world location $(x,y)$.

In the following chapter I also discuss how I model a localization algorithm. A
 concrete implementation could be use an object detection or segmentation network,
such as YOLO~\cite{redmon2018yolov3} or DeepLab~\cite{chen2017rethinking}, which are constantly
improving, to determine a bounding box for the vehicle. The center of the box in the pixel plane
can be seen as an estimate of the volumetric center of the vehicle. 
Using an estimate of the height of the vehicle, a geometric correction can be applied 
to obtain a good estimate of the vehicle position. %TODO maybe put this experiment in the Appendix?

However, real implementations could be arbitrary complex algorithms,
or even require vehicles carry visual markers.

\subsection{Coordinate Systems}
For reference, the following graphic shows the orientations
associated with camera placements. I only allow the roll and pitch
to be modified during my optimizations.

TODO FIGURE


\section{Curve Representations}

* discuss various ways of representing curves

* eg spline, linear approximations, dubins (what I've done), euler

* limitations

  * not smooth, more complex, don't generalize well to 3D (This is a bad one!)

* advantages

  * analytically easy to solve crosstrack distance, relatively accurate

  * less costly than splines

  * easy to look up time corresponding to any point in space (related to ease of crosstrack distance)

* unit testing for correctness - 3D geometry easy to mess up

\section{Vehicle Model and Navigation}

* Planar Z = 0 assumption, restrction to x,y,theta
* Kalman Filter
* odometry model, non-holonomic vehicles
* influence of vehicle and noise model on results TODO (=> discussion/conclusion?)

* PID, advantages and disadvantages

* Alternative controllers -- use of Stanford's steering controller and homebrew velocity controller.

\section{Objective Functions and Optimization}

* Localization versus Navigation
* Objection functions used in the past (=> related work? Or into camera placement chapter?) 

* grid search, hillclimbing..

\section{Submodularity}
* TODO -- leave to end to see if have time to explore this

\section{Mutual Information}



\chapter{Camera Model}
\label{chap:cameramodel}

The analysis presented in the following chapter is based on the navigaton of an
autonomous vehicle guided only by its own wheel odometry. 
This implies that errors accumulate over time, and
occasional updates need to be provided to reduce the positional 
uncertainty of the robot. The sensor selected to provide this
information is the camera, which is already widely deployed
in modern city infrustructure.

The updates to kalman filters contain two components: a value (in this case, an $(x,y)$ position),
along with an uncertainty in the form of covariance. This chapter
develops the simulated camera which is used to report vehicle 
location and uncertainty.
 
\section{Localization and Error}

\subsection{Approach}

In a real-world implementation of the proposed system, a camera would wirelessly
transmit estimated vehicle positions and associated uncertainty to vehicles in its field of view.

To enable position estimation, the camera needs to know its own world position and orientation, 
as well as the position and orientation of the ground plane. In in this work,
I assume the ground plane to be flat (a locally reasonable assumption, especially along roads), 
and equal to the $Z = 0$ plane. I report the position $(x,y)$ of the vehicle
as the estimated geometric center of the vehicle on the ground plane. 
Note that in practice, using the midpoint of the front or rear axle, rather than the center, 
may be a more consistent reference between vehicles.

Once the computer vision algorithm running on the camera estimates the center of the vehicle,
we must derive an uncertainty in the form of a covariance. I develop a mathematical model of localizaton error,
and analyze its properties. 

To start with, I develop a circular error radius, $r$, which I show to contain
the true error 99\% of the time. While circular error radii are more amenable to analysis and are good
estimates at high pitch angles, error ellipses are more realistic at low pitch angles and are an 
even more accurate error bound. Therefore, I also transform the circular error bounds into error ellipses at the end of this chaper.

Both circles and ellipses can be encoded as three standard deviations of a normal distrbution
using the covariance matrix for the estimated state $(x,y)$. The error radius directly determines
the operational capability of the autonomous vehicles, thus must be accurately represented.

I model three components that contribute to the error radius:
\begin{enumerate}
    \item Error in the camera's own location and orientation due to imprecise installation
    \item The algorithmic error in the estimation of the pixel that represents the estimated center of the vehicle
    \item The ground area covered by the pixel that represents the estimated center of the vehicle (encoding camera's finite precision)
\end{enumerate}
 
These will be analysed in turn. Note that I assume calibrated cameras without distortions, and no motion blur.


\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/a_estimate_example.pgf}
    \end{center}
    \caption[Example Predictions about a World Point]{The red point is a true world location, while green points were
    estimated by a large sample of cameras. Discrepancies occur due to the three errors listed above. We must associate an estimated error with each green point.}
    \label{fig:camera:dist}
\end{figure}



\subsection{Location and Orientation Error}

Of the three sources of error, this one is the most difficult to reason about. 
Many transport authorities have standard mounting specifications, including mount heights, for cameras~\cite{StreetscapeGuideance}.
However, none of these specify the tolerances in location and orientation, and we will have to make some educated guesses.

I assume that the position and orientation error is normally distributed, with
95\% of positions within 0.03 meters in any direction, 
and orientation within a 0.5 degree cone about the principal axis 95\% of the time.
In mathematical terms, this means $\Delta_{x}$, $\Delta_{y}$, and $\Delta_{z}$ 
are all sampled from $\mathcal{N}(0, (0.03/2)^2)$. The orientation error is given as
$\Delta_{orientation} \sim \mathcal{N}(0, (0.5/2)\degree2)$.
These tight bounds could be achieved using some sort of post-installation 
calibration procedure. 

% TODO discuss such a calibration procedure!

%Orientation accuracy can be further improved by adding 
%an interial measurement unit to the camera, which on its own
%could provide better than $0.5\degree$ accuracy in the roll and raw directions according
%to~\cite{kok2017}.

To translate these values into an uncertainty on the ground plane, consider that
any horizontal camera movement ($\Delta_{x}$, $\Delta_{y}$) from its true position simply translates
the predicted vehicle location and therefore increases its error by the length of the translation.
I call this combined value $err_{xy}$ and at two standard deviations it is bounded above by
$\abs{err_{xy}} = 0.03$.

\begin{figure}[hbh]
\centering
\begin{subfigure}[b]{.45\textwidth}
  \centering
  \resizebox{\linewidth}{!}{\input{figures/camera/xyz_error.pstex_t}} 
  \caption{Positional Errors}
  \label{fig:camera:xyz error}
\end{subfigure}%
\begin{subfigure}[b]{.45\textwidth}
  \centering
  \resizebox{\linewidth}{!}{\input{figures/camera/orient_error.pstex_t}}
  \caption{Orientational Error}
  \label{fig:camera:orient error}
\end{subfigure}
\label{fig:camera:errors}
\end{figure}
Vertical translation, $\Delta_z\sim \mathcal{N}(0, (0.03/2)^2)$
of the camera induces an error as $err_{z}(d, h) = \Delta_z \frac{d}{h}$ %, with variance $\sigma_{z}(d,h)^2 = (d/h)^2(0.03/2)^2$ 
where $d$ is the ground distance to the vehicle (on the XY plane), and $h$ is the true intended
height of the camera.

Thus, the total contribution from the camera's positional error to the error radius
will (95\% of the time) be bounded by $(err_{xy} + err_{z}(d, h))$, which evaluates to

\[ err_{xyz}(h, d) = 0.03 + \frac{0.03d}{h} \]

Similarly, it can be shown that using a conical error region about the desired principal axis
of magnitude $\Delta_{orientation} \sim \mathcal{N}(0,(0.5/2)\degree)$, 
for a camera at height $h$, and ground distance to the vehicle $d$, the
error induced in the estimation of the vehicle's center is 

\[ err_{orientation}(h, d) = h\tan(\tan^{-1}(d/h) + \Delta_{orientation}) - d \] %\frac{hd + h^2\Delta_{orientation}}{h - d\Delta_{orientation}} - d$.




\subsection{Algorithmic Error}

The vehicle localization algorithm needs to scan the pixel plane for vehicles,
and return pixel representing the center of vehicles on the ground plane.
The center of the pixel projected onto the ground plane is taken as the vehicle center.

This process can be arbitrarily complex. I assume that at the time of deployment, 
computer vision algorithms are very good at this task, but may still
be wrong by several pixels. I analyze the effect of this inaccuracy
in the model validation Section~\ref{sec:camera:validation} and refer to the Euclidean distance
in the pixel plane from the true center pixel to the calculated one
as $\Delta_{alg} \sim \mathcal{N}(0, \eta_{alg}/2)$.

\subsection{Error from Camera Limitations}

Lastly, the error induced by the camera must be accounted for. Consider that 
in the limit, a vehicle is covered by exactly one pixel, meaning any estimate
of its position is at least as uncertain as the ground area 
covered by that pixel. On the other hand, if a vehicle
exactly fills the camera plane and the localization algorithm perfectly determines
the center of the vehicle in the pixel plane, the estimate still cannot
be better than the area covered by the single pixel.

I call the area covered by a pixel $p$, $A(p)$. $A(p)$ is also a function of the camera
position and orientation, and the camera parameters (field of view and resolution),
but these are omitted here as they are constant for any given camera.

Correspondingly, an approximate upper bound on the longest distance
within a pixel can be calculated as the diagonal of a square,
resulting in a ground distance of $err_{pixel}(p) = \sqrt{(2A(p))}$. 


\subsection{Complete Error Model}

I now combine the three sources of error to determine an approximate
error radius for the vehicle localization.

\[
    r(h,d,p) = err_{xyz}(h,d) + err_{orientation}(h,d) + \Delta_{alg}*err_{pixel}(p)
\]

For any individual camera, the $err_{xyz}$ and $err_{orientation}$ terms introduce
a directed bias for any point in the pixel plane, though over many installed cameras
these terms have a normal distribution. The $\Delta_{alg}$ is 
modeled as normal as well. To take advantage of this, I also multiply each with a tuning
parameter, related to their variances, which I use to trade off error bound size versus
error bound accuracy. The $err_{pixel}$ term is a systematic 
and complex function of camera parameters and estimated vehicle location, and
the only component with a non-zero mean over a large number of samples.

To use $r$ in simulation, when a vehicle is visible to a camera,
I project the ground truth vehicle center to the (slightly-mispositioned) camera, which
automatically incorporates the positional and orientational errors.
The algorithmic error shifts the incident pixel a distance sampled according to the distribution 
of $\Delta_{alg}$, in a randomly sampled direction. 
Finally, the pixel error is computed from the ground area of the chosen pixel.

\section{Error Model Validation}
\label{sec:camera:validation}

In this section I evaluate the error radius function developed above. It is a good error model if,

\begin{enumerate}
    \item For a given estimated vehicle location, the true location should be within the predicted
          error radius most of the time. My formulation aims for 95\% accuracy.
    \item It is not over-conservative: the tighter the bound, the more useful the update is
          for vehicle navigation.
\end{enumerate}

I also examine how the different components that make up the error radius vary
as the pitch of the camera varies, followed by a brief look at the
effect of algorithmic inaccuracy. Experiments were performed
with the camera placed at a height of 6 meters above the ground, and a fixed
field of view of 60 degrees horizontally and vertically. Rotation about the Z axis
was ignored as it does not affect results.

\subsection{Quality of $r$ as an Error Bound}

For this and the following subsection I assume a perfect vision algorithm,
setting $\Delta_{alg}$ to 1.

\begin{table}[htb]
    \centering
    \caption[$r$ as an Error Bound]{Success Rate of $r$ as an Error Bound}
    \label{tab:camera:bound accuracy}
    \begin{tabular}{@{}ccc@{}}
        \toprule
        Total Samples & \# Within Error Bound & \% Within Error Bound \\ \midrule
        125000              & 124290                  & 99.432 
    \end{tabular}
\end{table}

Table~\ref{tab:camera:bound accuracy} shows that more than 99\% of 125000 sampled camera positions
and vehicle positions, the distance from the predicted vehicle position to 
its actual position is within the calculated error radius. This
is actually better than intended, since the construction was for 95\% accuracy.

It is easy to achieve an error bound that is always valid: simply set it very high.
Figure \ref{fig:camera:diff bound error} shows two important characteristics. Firstly, the difference between
the predicted error radius and the actual error distance is small, meaning
it is not an excessively high bound. Secondly, when the bound is \textit{incorrect} (very small region less than zero in blue)
the true error only slightly exceeds the error bound.

\begin{figure}[htb]
    \begin{center}
        %\input{figures/camera/diff_radius_true_error.pgf}
        \includegraphics{figures/camera/diff_radius_true_error.pdf}
    \end{center}
    \caption[Bound Minus True Distance]{Computed radius minus actual error distance.}
    \label{fig:camera:diff bound error}
\end{figure}

\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/proportion_radius_true_error.pgf}
    \end{center}
    \caption[Bound Minus True Distance as a Proportion]{Radius error bound minus actual error distance as a proportion of radius.}
    \label{fig:camera:proportional diff}
\end{figure}

To look at how tight the error radius computation is, I present the same chart
but plot the residual between predicted radius and actual error distance as a 
proportion of the predicted radius. Figure~\ref{fig:camera:proportional diff} shows that for the majority
of predictions, the error radius is far too large -- most commonly it is 3 times 
too large. 

Note that the true error is dependent on several random variables -- sampled positional and orientational
error, and pertubations modeling algorithmic error (pixel ground area is not randomly sampled). Each
of these are approximately gaussian and added together in absolute values, meaning the computed error bound is intended to be in the tail
of a rectified gaussian distribution as well. Thus, we expect most samples to
fall far from the error bound, which is exactly what we see.
Attempts to make the bound tighter resulted in a loss of the 99\% accuracy.

These results suggests the $r$ is good at modeling the true error, even though during construction
I did not model interplay between some of the factors (eg. how orientation 
or positional error affects pixel area inaccuracy).

\subsection{Error Radius as a function of Pitch Angle}

The \textit{XY} offset of a camera induces a constant distance on the ground plane. However,
all other components modeled in the error bound are dependent on pitch angle. Notably,
pixel areas diverge as the camera angle approaches horizontal. Figure~\ref{fig:camera:radius versus pitch}
illustrates the components of the error radius as a function of pitch.

\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/mean_error_pitch_variation.pgf}
    \end{center}
    \caption[$r$ as a Function of Pitch Angle]{How $r$ varies as a function pitch angle. Error bars are left out for clarity.}
    \label{fig:camera:radius versus pitch}
\end{figure}


This gives us an intuition for how each factor impacts the radius.
As expected, orientation and pixel area contributions
come to dominate, while at $90\degree$, when the camera is pointing straight down, the dominant
error is the camera's horizontal translation.

Also note that the pixel-area component of the error is the only component that
can be affected by the camera's configuration and the algorithmic 
accuracy in detecting the center pixel of the vehicle. This leads
to some fundamental limits discussed in Section~\ref{sec:camera:implications}.

\subsection{Impact of Algorithmic Inaccuracy}

The prior subsections were all presented without any algorithmic inaccuracies.
However, no computer vision algorithm can be expected to perfectly determine
the pixel containing the center of the vehicle.

\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/algorithm_influence.pgf}
    \end{center}
    \caption[Effect of Algorithmic Inaccuracy]{The effect of algorithmic inaccuracy on the mean value and accuracy of $r$. Error bars indicate two standard deviations.}
    \label{fig:camera:algorithm effect}
\end{figure}


Figure~\ref{fig:camera:algorithm effect} shows how the mean error radius changes as a function of the 
algorithmic error in red. In blue, I show that the accuracy as a bound is minimally influenced 
by the introduction of the extra error source. This implies that my bound of
algorithmic error is indeed well matched to the growth of the actual error.


\section{Limits and Implications}
\label{sec:camera:implications}

Having validated that the error model serves as a good bound on the real errors, we can use the function $r$
to reason about fundamental limits of the proposed system.

One interesting questions to ask is: if we had infinite resolution,
how well could we perform? Alternatively, how much accuracy do we gain from increasing resolution?

\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/resolution_angle_effects_mean.pgf}
    \end{center}
    \caption[Resolution Limits]{How increased resolution can increase the error bound at different pitch angles.}
    \label{fig:camera:resolution}
\end{figure}

Figure~\ref{fig:camera:resolution} shows that the mean error bound stops decreasing
significantly beyond a resolution of 2000 by 2000 pixels. A smaller pitch 
angle gains further from higher resolution, which is in line with what we 
expect from Figure~\ref{fig:camera:radius versus pitch} where we saw that more horizontal cameras
have a larger part of their error bound stemming from pixel area uncertainty,
which can in turn be reduced further by increasing resolution.

A similar behavior can be had by decreasing the field of view, which was for
these experiments set to $60\degree$ horizontally and vertically. 
In the next chapter, I fix the resolution and only optimize for the field of view,
since a higher resolution always increases the performance
of the proposed system at no cost and is thus not interesting.

Figure~\ref{fig:camera:resolution} was created assuming the following 95\% bounds: positional error
within 3 centimeters in any direction, orientational error within a
$5\degree$ diameter cone about the intended axis, 
and an algorithmic inaccuracy radius of 4 pixels. Table \ref{tab:camera:best error bounds}
shows the tightest reasonable bounds (sampled from
a large number of cameras instantiated according to the allowed variations,
and across various world vehicle positions). 

% TODO talk about sufficient accuracy required for navigation!!

%Angle: 90, Mean radius across resolutions: [0.40197440201387813, 0.19293124638700396, 0.15081008417570249, 0.11928250253476221, 0.10350244344841893, 0.094265799817414259, 0.09135945934363611]
%Angle: 70, Mean radius across resolutions: [0.45010524499283522, 0.21527710378253404, 0.16885180262701374, 0.13411132016861682, 0.11625843106774317, 0.10492112594687218, 0.10059377863697491]
%Angle: 50, Mean radius across resolutions: [0.70077300652909458, 0.33235179394953457, 0.26343141230548817, 0.20505057188734008, 0.1824444301888852, 0.16520421425770482, 0.15808103541200413]
%Angle: 40, Mean radius across resolutions: [1.19613352368245, 0.57308821388154529, 0.44891176913717401, 0.35664516588458312, 0.31671915528379335, 0.28935555994940343, 0.29652864029514137]
%Angle: 90, Median radius across resolutions: [0.4021706919463145, 0.19317966321015673, 0.15129084585686103, 0.11946773675605801, 0.10354271824198089, 0.094174543733546487, 0.091384413508090304]
%Angle: 70, Median radius across resolutions: [0.43152828692038625, 0.20319800518188907, 0.15991538828760138, 0.12556952323628817, 0.10753734230030643, 0.097301716687867551, 0.094192860259294769]
%Angle: 50, Median radius across resolutions: [0.5812734705835072, 0.27710777940867815, 0.20624173044352512, 0.16315340157784369, 0.14536687331250048, 0.13022587786212858, 0.12444617233290693]
%Angle: 40, Median radius across resolutions: [0.74747875637696892, 0.36061541520381596, 0.27549923065859278, 0.21482443305607002, 0.18843698803004749, 0.16809772152143937, 0.17067692688862446]
%Angle: 90, 99th percentile radius across resolutions: [0.42027722082110963, 0.20973881691227797, 0.16802942748374572, 0.13753351941756201, 0.12153702404932258, 0.11220089853368999, 0.1092350371485367]
%Angle: 70, 99th percentile radius across resolutions: [0.64096122009177803, 0.31288726892431312, 0.24971344961445791, 0.20086634699326877, 0.17589238199538776, 0.16510546392126713, 0.16068880262436383]
%Angle: 50, 99th percentile radius across resolutions: [1.6482785424932636, 0.78631792753850227, 0.64515988844698235, 0.51926067759233085, 0.44866023607367178, 0.42721999031097868, 0.40268379296792561]
%Angle: 40, 99th percentile radius across resolutions: [4.6372525003137337, 2.3852774525327942, 1.9460014599579245, 1.6544023827949428, 1.3719493458703107, 1.3728843421688703, 1.3448926870675881]


\begin{table}[htb]
    \centering
    \caption{Sampled Prediction Error Bounds using 2000x2000 resolution}
    \label{tab:camera:best error bounds}
    \begin{tabular}{@{}ccccc@{}}
        \toprule 
        \textbf{Pitch}       & \textbf{Mean (m)}   & \textbf{Median (m)} & \textbf{$95^{th}$ Percentile (m)} &  \\ \midrule 
        $90\degree$ & \textbf{0.1035} & \textbf{0.1035} & \textbf{0.1172}           &  \\ 
        $70\degree$ & \textbf{0.1163} & \textbf{0.1075} & \textbf{0.1658}           &  \\
        $50\degree$ & \textbf{0.1824} & \textbf{0.1454} & 0.3866           &  \\
        $40\degree$ & 0.3167 & \textbf{0.1884} & 1.022            &  \\ \midrule
    \end{tabular}
\end{table}


As discussed in the related works chapter, allowable lateral deviations are generally
quoted at $\pm$10-25cm. The table above shows in bold which camera placements would be 
within these limits. Note that even though the mean and 95\% percentile error bounds
at lower pitch angles are high, the median tells us that most measurements
from such a camera would still be useful.

Lastly, I present TODO

\begin{table}[htb]
\centering
\caption{Median 99\% Localization Bound for 70$\degree$ pitch, 2000x2000 pixels}
\label{tab:camera:70deg}

\begin{tabular}{ccccccccc}
\toprule
& \multicolumn{8}{c}{\textbf{Orientation Error, 95\%}} \\
\cmidrule(ll){2-9}
\multicolumn{1}{N}{\textbf{95\% Pos. Error (m)}} & 0$\degree$& 0.2$\degree$& 0.4$\degree$ & 0.6$\degree$ & 0.8$\degree$ & 1$\degree$ & 1.2$\degree$ & 1.4$\degree$\\
\cmidrule(lr){1-1}
\cmidrule(ll){2-9}
0.0 & \textbf{0.017} & \textbf{0.034} & \textbf{0.051} & \textbf{0.069} & \textbf{0.087} & \textbf{0.103} & \textbf{0.124} & \textbf{0.140} \\
0.1 & \textbf{0.177} & \textbf{0.196} & \textbf{0.214} & \textbf{0.230} & \textbf{0.248} & \textbf{0.268} & \textbf{0.280} & \textbf{0.298 } \\
0.2 & 0.342 & 0.360 & 0.378 & 0.394 & 0.409 & 0.430 & 0.443 & 0.460  \\
0.2 & 0.503 & 0.524 & 0.538 & 0.552 & 0.566 & 0.586 & 0.605 & 0.616  \\
0.4 & 0.665 & 0.685 & 0.702 & 0.715 & 0.737 & 0.757 & 0.765 & 0.794  \\
\end{tabular} 
\end{table}


\begin{table}[htb]
\centering
\caption{Median 99\% Localization Bound for 45$\degree$ pitch, 2000x2000 pixels}
\label{tab:camera:70deg}

\begin{tabular}{ccccccccc}
\toprule
& \multicolumn{8}{c}{\textbf{Orientation Error, 95\%}} \\
\cmidrule(ll){2-9}
\multicolumn{1}{N}{\textbf{95\% Pos. Error (m)}} & 0$\degree$& 0.2$\degree$& 0.4$\degree$ & 0.6$\degree$ & 0.8$\degree$ & 1$\degree$ & 1.2$\degree$ & 1.4$\degree$\\
\cmidrule(lr){1-1}
\cmidrule(ll){2-9}
0 & \textbf{0.027} & \textbf{0.060} & \textbf{0.083} & \textbf{0.114} & \textbf{0.145} &\textbf{ 0}.180 & \textbf{0.211} & \textbf{0.228} \\
0.1 & \textbf{0.222} & \textbf{0.252} & \textbf{0.285} & 0.324 & 0.338 & 0.365 & 0.398 & 0.446 \\
0.2 & 0.421 & 0.460 & 0.484 & 0.504 & 0.540 & 0.561 & 0.599 & 0.636 \\
0.3 & 0.624 & 0.661 & 0.682 & 0.713 & 0.761 & 0.764 & 0.796 & 0.837 \\
0.4 & 0.839 & 0.828 & 0.873 & 0.921 & 0.950 & 0.971 & 1.030 & 1.030 \\
\end{tabular} 
\end{table}
\begin{tabular}{ccccccccc}
\end{tabular} 



\section{Error Circles to Error Ellipses}

The prior sections used circular error bounds, which while also accurate and insightful,
can be slightly improved upon. A quick inspection of position estimates versus their true
positions in Figure~\ref{fig:camera:distributions} shows that at higher pitch angles, the estimates lie 
along an oval rather than a circle.

\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/actual_position_estimate_distributions.pgf}
    \end{center}
    \caption[Distribution of Estimates]{Ground plane estimates (green) of true position (red) across a large sample of cameras showing algorithmic, positional, and orientational error.}
    \label{fig:camera:distributions}
\end{figure}

I thus reshape the error circles into ovals of equal area, with the major
and minor axes determined by the ratio of ground height to width of the chosen pixel.
The orientation of the ellipse is given by the angle from the camera to the pixel's
ground position.


\begin{figure}[htb]
    \begin{center}
        \input{figures/camera/ovals_vs_circles.pgf}
    \end{center}
    \caption[elliptical versus circular error]{An example of where elliptical error bounds are more accurate than circular ones. Red circles represent error circles, while error ellipses are in blue. Note the vast majority of both are accurate bounds (very lightly drawn ovals and circles)}
    \label{fig:camera:oval better}
\end{figure}

This modification improves the accuracy of the error bound even further, as shown
in Table~\ref{tab:camera:ellipse vs circle}. A case of this occuring is shown
in Figure~\ref{fig:camera:oval better} which samples many camera positions and orientations
estimating the location of the point in red. The circular and elliptical 
error bounds that contain the red point are shown faintly in the background. One
case where the circular error bound fails while the elliptical one succeeds
is shown in bold.

\begin{table}[htb]
    \centering
    \caption[Elliptical versus Circular Error Bound]{$r$ as an Elliptical versus Circular Error Bound}
    \label{tab:camera:ellipse vs circle}
    \begin{tabular}{@{}llll@{}}
        \toprule
        Bound Type    & \# Samples  & \# Within Error& \% Within Error \\ \midrule
        Circular      &   125000              & 124290                  & 99.432  \\
        Elliptical    &   125000              & 124528                  & \textbf{99.622}
    \end{tabular}
\end{table}


In simulation I use oval error bounds as these more effectively represent the 
actual shape of errors, encoding the angle and lengths of major
and minor axes into the covariance between $x$ and $y$.

\section{Conclusion}

This chapter has developed and evaluated the camera sensor model
used in the following chapter to provide correctional updates
to an autonomous vehicle. To achieve this, the center
of the vehicle needs to be estimated in world coordinates. This
is associated with an error, which informs how trustworthy
an individual update is.

The error bound developed here, $r$, is a function of camera height,
camera pixel areas, and the horizontal distance to the vehicle center.
It was shown to be an effective upper bound, 99\% of the time, and
small enough to be usable in a real-world implementation (assuming
the installation bounds can be achieved). The effect of pitch angle
was extensively analyzed, and Section~\ref{sec:camera:implications}
explored the impact of resolution in the limit, providing a 
fundamental limit on the effectiveness of the proposed system,
as well as some real-world requirements in terms of
installation and algorithmic accuracy. Finally, I showed
the effectiveness of reshaping circular bounds into elliptical ones.


\chapter{Camera Placement}
\label{chap:cameraplacement}


\section{Single Camera Placement -- Localization}

\subsection{A Cheaper Objective Function}
Pros/Cons

\subsection{Straight Roads}

\subsection{Relation to Curvature}

\subsection{Aside: Intersection}

*** optional -- examine if time remaining at the end ***

Because we can in this case! ... may be a pain to implement in current code of piecewise paths...

\subsection{Varying Pitch, Yaw, and Horizontal and Vertical FoV}

* hill climbing (put in background?)


\section{Single Camera Placement -- Navigation}

\subsection{Full Objection Function}
* Monte Carlo approximation of entropy

* may need to outline maximizing mutual information =>

* to minimizing entropy. Relate to Beinhofer's evaluation

* TODO Try out just using trace of covariance matrix...

* optional: research other localization/navigation objectives used again, related log(det) to tr(cov) to error radius to entropy...
\subsection{Straight Roads}

\subsection{Relation to Curvature}

\subsection{Comparison to Simpler Objection Function}



\section{Multiple Camera Placement}
* Discuss submodularity

* test submodularity by sampling -- make sure minimum overlap of camera ground areas!

* straight roads + 1 curvature

* propose cascaded optimization -- choose possible sensor locations, use cheaper objective to find top 1 or 2 
positions or orientations, add these to the set of possible locations and orientations



\section{Conclusion}

\chapter{Optimizing Road Construction for Navigation}

* brief - look at how to connect two points if various obstacles are put in the way
* maybe try a triangular point an and rectangular block 


\chapter{Summary and Conclusion} 




\appendix
\singlespacing

\printbibliography

\end{document}
